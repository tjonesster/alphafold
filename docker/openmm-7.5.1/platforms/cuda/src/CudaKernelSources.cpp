/* -------------------------------------------------------------------------- *
 *                                   OpenMM                                   *
 * -------------------------------------------------------------------------- *
 * This is part of the OpenMM molecular simulation toolkit originating from   *
 * Simbios, the NIH National Center for Physics-Based Simulation of           *
 * Biological Structures at Stanford, funded under the NIH Roadmap for        *
 * Medical Research, grant U54 GM072970. See https://simtk.org.               *
 *                                                                            *
 * Portions copyright (c) 2012 Stanford University and the Authors.           *
 * Authors: Peter Eastman                                                     *
 * Contributors:                                                              *
 *                                                                            *
 * This program is free software: you can redistribute it and/or modify       *
 * it under the terms of the GNU Lesser General Public License as published   *
 * by the Free Software Foundation, either version 3 of the License, or       *
 * (at your option) any later version.                                        *
 *                                                                            *
 * This program is distributed in the hope that it will be useful,            *
 * but WITHOUT ANY WARRANTY; without even the implied warranty of             *
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              *
 * GNU Lesser General Public License for more details.                        *
 *                                                                            *
 * You should have received a copy of the GNU Lesser General Public License   *
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.      *
 * -------------------------------------------------------------------------- */

#include "CudaKernelSources.h"

using namespace OpenMM;
using namespace std;

const string CudaKernelSources::angleForce = "real3 v0 = make_real3(pos2.x-pos1.x, pos2.y-pos1.y, pos2.z-pos1.z);\n"
"real3 v1 = make_real3(pos2.x-pos3.x, pos2.y-pos3.y, pos2.z-pos3.z);\n"
"#if APPLY_PERIODIC\n"
"APPLY_PERIODIC_TO_DELTA(v0)\n"
"APPLY_PERIODIC_TO_DELTA(v1)\n"
"#endif\n"
"real3 cp = cross(v0, v1);\n"
"real rp = cp.x*cp.x + cp.y*cp.y + cp.z*cp.z;\n"
"rp = max(SQRT(rp), (real) 1.0e-06f);\n"
"real r21 = v0.x*v0.x + v0.y*v0.y + v0.z*v0.z;\n"
"real r23 = v1.x*v1.x + v1.y*v1.y + v1.z*v1.z;\n"
"real dot = v0.x*v1.x + v0.y*v1.y + v0.z*v1.z;\n"
"real cosine = min(max(dot*RSQRT(r21*r23), (real) -1), (real) 1);\n"
"real theta = ACOS(cosine);\n"
"COMPUTE_FORCE\n"
"real3 force1 = cross(v0, cp)*(dEdAngle/(r21*rp));\n"
"real3 force3 = cross(cp, v1)*(dEdAngle/(r23*rp));\n"
"real3 force2 = -force1-force3;\n"
"";
const string CudaKernelSources::bondForce = "real3 delta = make_real3(pos2.x-pos1.x, pos2.y-pos1.y, pos2.z-pos1.z);\n"
"#if APPLY_PERIODIC\n"
"APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"real r = SQRT(delta.x*delta.x + delta.y*delta.y + delta.z*delta.z);\n"
"COMPUTE_FORCE\n"
"dEdR = (r > 0) ? (dEdR / r) : 0;\n"
"delta *= dEdR;\n"
"real3 force1 = delta;\n"
"real3 force2 = -delta;\n"
"";
const string CudaKernelSources::common = "/**\n"
" * This file contains CUDA definitions for the macros and functions needed for the\n"
" * common compute framework.\n"
" */\n"
"\n"
"#define KERNEL extern \"C\" __global__\n"
"#define DEVICE __device__\n"
"#define LOCAL __shared__\n"
"#define LOCAL_ARG\n"
"#define GLOBAL\n"
"#define RESTRICT __restrict__\n"
"#define LOCAL_ID threadIdx.x\n"
"#define LOCAL_SIZE blockDim.x\n"
"#define GLOBAL_ID (blockIdx.x*blockDim.x+threadIdx.x)\n"
"#define GLOBAL_SIZE (blockDim.x*gridDim.x)\n"
"#define GROUP_ID blockIdx.x\n"
"#define NUM_GROUPS gridDim.x\n"
"#define SYNC_THREADS __syncthreads();\n"
"#define MEM_FENCE __threadfence_block();\n"
"#define ATOMIC_ADD(dest, value) atomicAdd(dest, value)\n"
"\n"
"typedef long long mm_long;\n"
"typedef unsigned long long mm_ulong;\n"
"\n"
"#define SUPPORTS_64_BIT_ATOMICS 1\n"
"#define SUPPORTS_DOUBLE_PRECISION 1\n"
"";
const string CudaKernelSources::constraints = "extern \"C\" __global__ void applyPositionDeltas(int numAtoms, real4* __restrict__ posq, real4* __restrict__ posqCorrection, mixed4* __restrict__ posDelta) {\n"
"    for (unsigned int index = blockIdx.x*blockDim.x+threadIdx.x; index < numAtoms; index += blockDim.x*gridDim.x) {\n"
"#ifdef USE_MIXED_PRECISION\n"
"        real4 pos1 = posq[index];\n"
"        real4 pos2 = posqCorrection[index];\n"
"        mixed4 pos = make_mixed4(pos1.x+(mixed)pos2.x, pos1.y+(mixed)pos2.y, pos1.z+(mixed)pos2.z, pos1.w);\n"
"#else\n"
"        mixed4 pos = posq[index];\n"
"#endif\n"
"        pos.x += posDelta[index].x;\n"
"        pos.y += posDelta[index].y;\n"
"        pos.z += posDelta[index].z;\n"
"#ifdef USE_MIXED_PRECISION\n"
"        posq[index] = make_real4((real) pos.x, (real) pos.y, (real) pos.z, (real) pos.w);\n"
"        posqCorrection[index] = make_real4(pos.x-(real) pos.x, pos.y-(real) pos.y, pos.z-(real) pos.z, 0);\n"
"#else\n"
"        posq[index] = pos;\n"
"#endif\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::coulombLennardJones = "{\n"
"#if USE_EWALD\n"
"    unsigned int includeInteraction = (!isExcluded && r2 < CUTOFF_SQUARED);\n"
"    const real alphaR = EWALD_ALPHA*r;\n"
"    const real expAlphaRSqr = EXP(-alphaR*alphaR);\n"
"#if HAS_COULOMB\n"
"    const real prefactor = ONE_4PI_EPS0*CHARGE1*CHARGE2*invR;\n"
"#else\n"
"    const real prefactor = 0.0f;\n"
"#endif\n"
"\n"
"#ifdef USE_DOUBLE_PRECISION\n"
"    const real erfcAlphaR = erfc(alphaR);\n"
"#else\n"
"    // This approximation for erfc is from Abramowitz and Stegun (1964) p. 299.  They cite the following as\n"
"    // the original source: C. Hastings, Jr., Approximations for Digital Computers (1955).  It has a maximum\n"
"    // error of 1.5e-7.\n"
"\n"
"    const real t = RECIP(1.0f+0.3275911f*alphaR);\n"
"    const real erfcAlphaR = (0.254829592f+(-0.284496736f+(1.421413741f+(-1.453152027f+1.061405429f*t)*t)*t)*t)*t*expAlphaRSqr;\n"
"#endif\n"
"    real tempForce = 0.0f;\n"
"#if HAS_LENNARD_JONES\n"
"    real sig = SIGMA_EPSILON1.x + SIGMA_EPSILON2.x;\n"
"    real sig2 = invR*sig;\n"
"    sig2 *= sig2;\n"
"    real sig6 = sig2*sig2*sig2;\n"
"    real eps = SIGMA_EPSILON1.y*SIGMA_EPSILON2.y;\n"
"    real epssig6 = sig6*eps;\n"
"    tempForce = epssig6*(12.0f*sig6 - 6.0f);\n"
"    real ljEnergy = epssig6*(sig6 - 1.0f);\n"
"    #if USE_LJ_SWITCH\n"
"    if (r > LJ_SWITCH_CUTOFF) {\n"
"        real x = r-LJ_SWITCH_CUTOFF;\n"
"        real switchValue = 1+x*x*x*(LJ_SWITCH_C3+x*(LJ_SWITCH_C4+x*LJ_SWITCH_C5));\n"
"        real switchDeriv = x*x*(3*LJ_SWITCH_C3+x*(4*LJ_SWITCH_C4+x*5*LJ_SWITCH_C5));\n"
"        tempForce = tempForce*switchValue - ljEnergy*switchDeriv*r;\n"
"        ljEnergy *= switchValue;\n"
"    }\n"
"    #endif\n"
"#if DO_LJPME\n"
"    // The multiplicative term to correct for the multiplicative terms that are always\n"
"    // present in reciprocal space.\n"
"    const real dispersionAlphaR = EWALD_DISPERSION_ALPHA*r;\n"
"    const real dar2 = dispersionAlphaR*dispersionAlphaR;\n"
"    const real dar4 = dar2*dar2;\n"
"    const real dar6 = dar4*dar2;\n"
"    const real invR2 = invR*invR;\n"
"    const real expDar2 = EXP(-dar2);\n"
"    const float2 sigExpProd = SIGMA_EPSILON1*SIGMA_EPSILON2;\n"
"    const real c6 = 64*sigExpProd.x*sigExpProd.x*sigExpProd.x*sigExpProd.y;\n"
"    const real coef = invR2*invR2*invR2*c6;\n"
"    const real eprefac = 1.0f + dar2 + 0.5f*dar4;\n"
"    const real dprefac = eprefac + dar6/6.0f;\n"
"    // The multiplicative grid term\n"
"    ljEnergy += coef*(1.0f - expDar2*eprefac);\n"
"    tempForce += 6.0f*coef*(1.0f - expDar2*dprefac);\n"
"    // The potential shift accounts for the step at the cutoff introduced by the\n"
"    // transition from additive to multiplicative combintion rules and is only\n"
"    // needed for the real (not excluded) terms.  By addin these terms to ljEnergy\n"
"    // instead of tempEnergy here, the includeInteraction mask is correctly applied.\n"
"    sig2 = sig*sig;\n"
"    sig6 = sig2*sig2*sig2*INVCUT6;\n"
"    epssig6 = eps*sig6;\n"
"    // The additive part of the potential shift\n"
"    ljEnergy += epssig6*(1.0f - sig6);\n"
"    // The multiplicative part of the potential shift\n"
"    ljEnergy += MULTSHIFT6*c6;\n"
"#endif\n"
"    tempForce += prefactor*(erfcAlphaR+alphaR*expAlphaRSqr*TWO_OVER_SQRT_PI);\n"
"    tempEnergy += includeInteraction ? ljEnergy + prefactor*erfcAlphaR : 0;\n"
"#else\n"
"    tempForce = prefactor*(erfcAlphaR+alphaR*expAlphaRSqr*TWO_OVER_SQRT_PI);\n"
"    tempEnergy += includeInteraction ? prefactor*erfcAlphaR : 0;\n"
"#endif\n"
"    dEdR += includeInteraction ? tempForce*invR*invR : 0;\n"
"#else\n"
"#ifdef USE_CUTOFF\n"
"    unsigned int includeInteraction = (!isExcluded && r2 < CUTOFF_SQUARED);\n"
"#else\n"
"    unsigned int includeInteraction = (!isExcluded);\n"
"#endif\n"
"    real tempForce = 0.0f;\n"
"#if HAS_LENNARD_JONES\n"
"    real sig = SIGMA_EPSILON1.x + SIGMA_EPSILON2.x;\n"
"    real sig2 = invR*sig;\n"
"    sig2 *= sig2;\n"
"    real sig6 = sig2*sig2*sig2;\n"
"    real epssig6 = sig6*(SIGMA_EPSILON1.y*SIGMA_EPSILON2.y);\n"
"    tempForce = epssig6*(12.0f*sig6 - 6.0f);\n"
"    real ljEnergy = includeInteraction ? epssig6*(sig6 - 1) : 0;\n"
"    #if USE_LJ_SWITCH\n"
"    if (r > LJ_SWITCH_CUTOFF) {\n"
"        real x = r-LJ_SWITCH_CUTOFF;\n"
"        real switchValue = 1+x*x*x*(LJ_SWITCH_C3+x*(LJ_SWITCH_C4+x*LJ_SWITCH_C5));\n"
"        real switchDeriv = x*x*(3*LJ_SWITCH_C3+x*(4*LJ_SWITCH_C4+x*5*LJ_SWITCH_C5));\n"
"        tempForce = tempForce*switchValue - ljEnergy*switchDeriv*r;\n"
"        ljEnergy *= switchValue;\n"
"    }\n"
"    #endif\n"
"    tempEnergy += ljEnergy;\n"
"#endif\n"
"#if HAS_COULOMB\n"
"  #ifdef USE_CUTOFF\n"
"    const real prefactor = ONE_4PI_EPS0*CHARGE1*CHARGE2;\n"
"    tempForce += prefactor*(invR - 2.0f*REACTION_FIELD_K*r2);\n"
"    tempEnergy += includeInteraction ? prefactor*(invR + REACTION_FIELD_K*r2 - REACTION_FIELD_C) : 0;\n"
"  #else\n"
"    const real prefactor = ONE_4PI_EPS0*CHARGE1*CHARGE2*invR;\n"
"    tempForce += prefactor;\n"
"    tempEnergy += includeInteraction ? prefactor : 0;\n"
"  #endif\n"
"#endif\n"
"    dEdR += includeInteraction ? tempForce*invR*invR : 0;\n"
"#endif\n"
"}\n"
"";
const string CudaKernelSources::customCVForce = "/**\n"
" * Copy the positions and velocities to the inner context.\n"
" */\n"
"extern \"C\" __global__ void copyState(real4* posq, real4* posqCorrection, mixed4* velm, int* __restrict__ atomOrder,\n"
"        real4* innerPosq, real4* innerPosqCorrection, mixed4* innerVelm, int* __restrict__ innerInvAtomOrder,\n"
"        int numAtoms) {\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numAtoms; i += blockDim.x*gridDim.x) {\n"
"        int index = innerInvAtomOrder[atomOrder[i]];\n"
"        innerPosq[index] = posq[i];\n"
"        innerVelm[index] = velm[i];\n"
"#ifdef USE_MIXED_PRECISION\n"
"        innerPosqCorrection[index] = posqCorrection[i];\n"
"#endif\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Copy the forces back to the main context.\n"
" */\n"
"extern \"C\" __global__ void copyForces(long long* forces, int* __restrict__ invAtomOrder, long long* innerForces,\n"
"        int* __restrict__ innerAtomOrder, int numAtoms, int paddedNumAtoms) {\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numAtoms; i += blockDim.x*gridDim.x) {\n"
"        int index = invAtomOrder[innerAtomOrder[i]];\n"
"        forces[index] = innerForces[i];\n"
"        forces[index+paddedNumAtoms] = innerForces[i+paddedNumAtoms];\n"
"        forces[index+paddedNumAtoms*2] = innerForces[i+paddedNumAtoms*2];\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Add all the forces from the CVs.\n"
" */\n"
"extern \"C\" __global__ void addForces(long long* forces, int bufferSize\n"
"    PARAMETER_ARGUMENTS) {\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < bufferSize; i += blockDim.x*gridDim.x) {\n"
"        ADD_FORCES\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::ewald = "__device__ real2 multofReal2(real2 a, real2 b) {\n"
"    return make_real2(a.x*b.x - a.y*b.y, a.x*b.y + a.y*b.x);\n"
"}\n"
"\n"
"/**\n"
" * Precompute the cosine and sine sums which appear in each force term.\n"
" */\n"
"\n"
"extern \"C\" __global__ void calculateEwaldCosSinSums(mixed* __restrict__ energyBuffer, const real4* __restrict__ posq, real2* __restrict__ cosSinSum, real4 periodicBoxSize) {\n"
"    const unsigned int ksizex = 2*KMAX_X-1;\n"
"    const unsigned int ksizey = 2*KMAX_Y-1;\n"
"    const unsigned int ksizez = 2*KMAX_Z-1;\n"
"    const unsigned int totalK = ksizex*ksizey*ksizez;\n"
"    real3 reciprocalBoxSize = make_real3(2*M_PI/periodicBoxSize.x, 2*M_PI/periodicBoxSize.y, 2*M_PI/periodicBoxSize.z);\n"
"    real reciprocalCoefficient = ONE_4PI_EPS0*4*M_PI/(periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"    unsigned int index = blockIdx.x*blockDim.x+threadIdx.x;\n"
"    mixed energy = 0;\n"
"    while (index < (KMAX_Y-1)*ksizez+KMAX_Z)\n"
"        index += blockDim.x*gridDim.x;\n"
"    while (index < totalK) {\n"
"        // Find the wave vector (kx, ky, kz) this index corresponds to.\n"
"\n"
"        int rx = index/(ksizey*ksizez);\n"
"        int remainder = index - rx*ksizey*ksizez;\n"
"        int ry = remainder/ksizez;\n"
"        int rz = remainder - ry*ksizez - KMAX_Z + 1;\n"
"        ry += -KMAX_Y + 1;\n"
"        real kx = rx*reciprocalBoxSize.x;\n"
"        real ky = ry*reciprocalBoxSize.y;\n"
"        real kz = rz*reciprocalBoxSize.z;\n"
"\n"
"        // Compute the sum for this wave vector.\n"
"\n"
"        real2 sum = make_real2(0);\n"
"        for (int atom = 0; atom < NUM_ATOMS; atom++) {\n"
"            real4 apos = posq[atom];\n"
"            real phase = apos.x*kx;\n"
"            real2 structureFactor = make_real2(COS(phase), SIN(phase));\n"
"            phase = apos.y*ky;\n"
"            structureFactor = multofReal2(structureFactor, make_real2(COS(phase), SIN(phase)));\n"
"            phase = apos.z*kz;\n"
"            structureFactor = multofReal2(structureFactor, make_real2(COS(phase), SIN(phase)));\n"
"            sum += apos.w*structureFactor;\n"
"        }\n"
"        cosSinSum[index] = sum;\n"
"\n"
"        // Compute the contribution to the energy.\n"
"\n"
"        real k2 = kx*kx + ky*ky + kz*kz;\n"
"        real ak = EXP(k2*EXP_COEFFICIENT) / k2;\n"
"        energy += reciprocalCoefficient*ak*(sum.x*sum.x + sum.y*sum.y);\n"
"        index += blockDim.x*gridDim.x;\n"
"    }\n"
"    energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] += energy;\n"
"}\n"
"\n"
"/**\n"
" * Compute the reciprocal space part of the Ewald force, using the precomputed sums from the\n"
" * previous routine.\n"
" */\n"
"\n"
"extern \"C\" __global__ void calculateEwaldForces(unsigned long long* __restrict__ forceBuffers, const real4* __restrict__ posq, const real2* __restrict__ cosSinSum, real4 periodicBoxSize) {\n"
"    unsigned int atom = blockIdx.x*blockDim.x+threadIdx.x;\n"
"    real3 reciprocalBoxSize = make_real3(2*M_PI/periodicBoxSize.x, 2*M_PI/periodicBoxSize.y, 2*M_PI/periodicBoxSize.z);\n"
"    real reciprocalCoefficient = ONE_4PI_EPS0*4*M_PI/(periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"    while (atom < NUM_ATOMS) {\n"
"        real3 force = make_real3(0);\n"
"        real4 apos = posq[atom];\n"
"\n"
"        // Loop over all wave vectors.\n"
"\n"
"        int lowry = 0;\n"
"        int lowrz = 1;\n"
"        for (int rx = 0; rx < KMAX_X; rx++) {\n"
"            real kx = rx*reciprocalBoxSize.x;\n"
"            for (int ry = lowry; ry < KMAX_Y; ry++) {\n"
"                real ky = ry*reciprocalBoxSize.y;\n"
"                real phase = apos.x*kx;\n"
"                real2 tab_xy = make_real2(COS(phase), SIN(phase));\n"
"                phase = apos.y*ky;\n"
"                tab_xy = multofReal2(tab_xy, make_real2(COS(phase), SIN(phase)));\n"
"                for (int rz = lowrz; rz < KMAX_Z; rz++) {\n"
"                    real kz = rz*reciprocalBoxSize.z;\n"
"\n"
"                    // Compute the force contribution of this wave vector.\n"
"\n"
"                    int index = rx*(KMAX_Y*2-1)*(KMAX_Z*2-1) + (ry+KMAX_Y-1)*(KMAX_Z*2-1) + (rz+KMAX_Z-1);\n"
"                    real k2 = kx*kx + ky*ky + kz*kz;\n"
"                    real ak = EXP(k2*EXP_COEFFICIENT)/k2;\n"
"                    phase = apos.z*kz;\n"
"                    real2 structureFactor = multofReal2(tab_xy, make_real2(COS(phase), SIN(phase)));\n"
"                    real2 sum = cosSinSum[index];\n"
"                    real dEdR = 2*reciprocalCoefficient*ak*apos.w*(sum.x*structureFactor.y - sum.y*structureFactor.x);\n"
"                    force.x += dEdR*kx;\n"
"                    force.y += dEdR*ky;\n"
"                    force.z += dEdR*kz;\n"
"                    lowrz = 1 - KMAX_Z;\n"
"                }\n"
"                lowry = 1 - KMAX_Y;\n"
"            }\n"
"        }\n"
"\n"
"        // Record the force on the atom.\n"
"\n"
"        atomicAdd(&forceBuffers[atom], static_cast<unsigned long long>((long long) (force.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.z*0x100000000)));\n"
"        atom += blockDim.x*gridDim.x;\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::fft = "static __inline__ __device__ real2 multiplyComplex(real2 c1, real2 c2) {\n"
"    return make_real2(c1.x*c2.x-c1.y*c2.y, c1.x*c2.y+c1.y*c2.x);\n"
"}\n"
"\n"
"/**\n"
" * Load a value from the half-complex grid produces by a real-to-complex transform.\n"
" */\n"
"static __inline__ __device__ real2 loadComplexValue(const real2* __restrict__ in, int x, int y, int z) {\n"
"    const int inputZSize = ZSIZE/2+1;\n"
"    if (z < inputZSize)\n"
"        return in[x*YSIZE*inputZSize+y*inputZSize+z];\n"
"    int xp = (x == 0 ? 0 : XSIZE-x);\n"
"    int yp = (y == 0 ? 0 : YSIZE-y);\n"
"    real2 value = in[xp*YSIZE*inputZSize+yp*inputZSize+(ZSIZE-z)];\n"
"    return make_real2(value.x, -value.y);\n"
"}\n"
"\n"
"/**\n"
" * Perform a 1D FFT on each row along one axis.\n"
" */\n"
"\n"
"extern \"C\" __global__ void execFFT(const INPUT_TYPE* __restrict__ in, OUTPUT_TYPE* __restrict__ out) {\n"
"    __shared__ real2 w[ZSIZE];\n"
"    __shared__ real2 data0[BLOCKS_PER_GROUP*ZSIZE];\n"
"    __shared__ real2 data1[BLOCKS_PER_GROUP*ZSIZE];\n"
"    for (int i = threadIdx.x; i < ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(-(SIGN)*i*2*M_PI/ZSIZE), sin(-(SIGN)*i*2*M_PI/ZSIZE));\n"
"    __syncthreads();\n"
"    \n"
"    const int block = threadIdx.x/THREADS_PER_BLOCK;\n"
"    for (int baseIndex = blockIdx.x*BLOCKS_PER_GROUP; baseIndex < XSIZE*YSIZE; baseIndex += gridDim.x*BLOCKS_PER_GROUP) {\n"
"        int index = baseIndex+block;\n"
"        int x = index/YSIZE;\n"
"        int y = index-x*YSIZE;\n"
"#if OUTPUT_IS_PACKED\n"
"        if (x < XSIZE/2+1) {\n"
"#endif\n"
"        if (index < XSIZE*YSIZE)\n"
"            for (int i = threadIdx.x-block*THREADS_PER_BLOCK; i < ZSIZE; i += THREADS_PER_BLOCK)\n"
"    #if INPUT_IS_REAL\n"
"                data0[i+block*ZSIZE] = make_real2(in[x*(YSIZE*ZSIZE)+y*ZSIZE+i], 0);\n"
"    #elif INPUT_IS_PACKED\n"
"                data0[i+block*ZSIZE] = loadComplexValue(in, x, y, i);\n"
"    #else\n"
"                data0[i+block*ZSIZE] = in[x*(YSIZE*ZSIZE)+y*ZSIZE+i];\n"
"    #endif\n"
"#if OUTPUT_IS_PACKED\n"
"        }\n"
"#endif\n"
"        __syncthreads();\n"
"        COMPUTE_FFT\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::fftR2C = "/**\n"
" * Combine the two halves of a real grid into a complex grid that is half as large.\n"
" */\n"
"extern \"C\" __global__ void packForwardData(const real* __restrict__ in, real2* __restrict__ out) {\n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"#if PACKED_AXIS == 0\n"
"        real2 value = make_real2(in[2*x*YSIZE*ZSIZE+y*ZSIZE+z], in[(2*x+1)*YSIZE*ZSIZE+y*ZSIZE+z]);\n"
"#elif PACKED_AXIS == 1\n"
"        real2 value = make_real2(in[x*YSIZE*ZSIZE+2*y*ZSIZE+z], in[x*YSIZE*ZSIZE+(2*y+1)*ZSIZE+z]);\n"
"#else\n"
"        real2 value = make_real2(in[x*YSIZE*ZSIZE+y*ZSIZE+2*z], in[x*YSIZE*ZSIZE+y*ZSIZE+(2*z+1)]);\n"
"#endif\n"
"        out[index] = value;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Split the transformed data back into a full sized, symmetric grid.\n"
" */\n"
"extern \"C\" __global__ void unpackForwardData(const real2* __restrict__ in, real2* __restrict__ out) {\n"
"    // Compute the phase factors.\n"
"    \n"
"#if PACKED_AXIS == 0\n"
"    __shared__ real2 w[PACKED_XSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_XSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/XSIZE), cos(i*2*M_PI/XSIZE));\n"
"#elif PACKED_AXIS == 1\n"
"    __shared__ real2 w[PACKED_YSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_YSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/YSIZE), cos(i*2*M_PI/YSIZE));\n"
"#else\n"
"    __shared__ real2 w[PACKED_ZSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(sin(i*2*M_PI/ZSIZE), cos(i*2*M_PI/ZSIZE));\n"
"#endif\n"
"    __syncthreads();\n"
"\n"
"    // Transform the data.\n"
"    \n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    const int outputZSize = ZSIZE/2+1;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        int xp = (x == 0 ? 0 : PACKED_XSIZE-x);\n"
"        int yp = (y == 0 ? 0 : PACKED_YSIZE-y);\n"
"        int zp = (z == 0 ? 0 : PACKED_ZSIZE-z);\n"
"        real2 z1 = in[x*PACKED_YSIZE*PACKED_ZSIZE+y*PACKED_ZSIZE+z];\n"
"        real2 z2 = in[xp*PACKED_YSIZE*PACKED_ZSIZE+yp*PACKED_ZSIZE+zp];\n"
"#if PACKED_AXIS == 0\n"
"        real2 wfac = w[x];\n"
"#elif PACKED_AXIS == 1\n"
"        real2 wfac = w[y];\n"
"#else\n"
"        real2 wfac = w[z];\n"
"#endif\n"
"        real2 output = make_real2((z1.x+z2.x - wfac.x*(z1.x-z2.x) + wfac.y*(z1.y+z2.y))/2, (z1.y-z2.y - wfac.y*(z1.x-z2.x) - wfac.x*(z1.y+z2.y))/2);\n"
"        if (z < outputZSize)\n"
"            out[x*YSIZE*outputZSize+y*outputZSize+z] = output;\n"
"        xp = (x == 0 ? 0 : XSIZE-x);\n"
"        yp = (y == 0 ? 0 : YSIZE-y);\n"
"        zp = (z == 0 ? 0 : ZSIZE-z);\n"
"        if (zp < outputZSize) {\n"
"#if PACKED_AXIS == 0\n"
"            if (x == 0)\n"
"                out[PACKED_XSIZE*YSIZE*outputZSize+yp*outputZSize+zp] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#elif PACKED_AXIS == 1\n"
"            if (y == 0)\n"
"                out[xp*YSIZE*outputZSize+PACKED_YSIZE*outputZSize+zp] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#else\n"
"            if (z == 0)\n"
"                out[xp*YSIZE*outputZSize+yp*outputZSize+PACKED_ZSIZE] = make_real2((z1.x-z1.y+z2.x-z2.y)/2, (-z1.x-z1.y+z2.x+z2.y)/2);\n"
"#endif\n"
"            else\n"
"                out[xp*YSIZE*outputZSize+yp*outputZSize+zp] = make_real2(output.x, -output.y);\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Load a value from the half-complex grid produced by a real-to-complex transform.\n"
" */\n"
"static __inline__ __device__ real2 loadComplexValue(const real2* __restrict__ in, int x, int y, int z) {\n"
"    const int inputZSize = ZSIZE/2+1;\n"
"    if (z < inputZSize)\n"
"        return in[x*YSIZE*inputZSize+y*inputZSize+z];\n"
"    int xp = (x == 0 ? 0 : XSIZE-x);\n"
"    int yp = (y == 0 ? 0 : YSIZE-y);\n"
"    real2 value = in[xp*YSIZE*inputZSize+yp*inputZSize+(ZSIZE-z)];\n"
"    return make_real2(value.x, -value.y);\n"
"}\n"
"\n"
"/**\n"
" * Repack the symmetric complex grid into one half as large in preparation for doing an inverse complex-to-real transform.\n"
" */\n"
"extern \"C\" __global__ void packBackwardData(const real2* __restrict__ in, real2* __restrict__ out) {\n"
"    // Compute the phase factors.\n"
"    \n"
"#if PACKED_AXIS == 0\n"
"    __shared__ real2 w[PACKED_XSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_XSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/XSIZE), sin(i*2*M_PI/XSIZE));\n"
"#elif PACKED_AXIS == 1\n"
"    __shared__ real2 w[PACKED_YSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_YSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/YSIZE), sin(i*2*M_PI/YSIZE));\n"
"#else\n"
"    __shared__ real2 w[PACKED_ZSIZE];\n"
"    for (int i = threadIdx.x; i < PACKED_ZSIZE; i += blockDim.x)\n"
"        w[i] = make_real2(cos(i*2*M_PI/ZSIZE), sin(i*2*M_PI/ZSIZE));\n"
"#endif\n"
"    __syncthreads();\n"
"\n"
"    // Transform the data.\n"
"    \n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        int xp = (x == 0 ? 0 : PACKED_XSIZE-x);\n"
"        int yp = (y == 0 ? 0 : PACKED_YSIZE-y);\n"
"        int zp = (z == 0 ? 0 : PACKED_ZSIZE-z);\n"
"        real2 z1 = loadComplexValue(in, x, y, z);\n"
"#if PACKED_AXIS == 0\n"
"        real2 wfac = w[x];\n"
"        real2 z2 = loadComplexValue(in, PACKED_XSIZE-x, yp, zp);\n"
"#elif PACKED_AXIS == 1\n"
"        real2 wfac = w[y];\n"
"        real2 z2 = loadComplexValue(in, xp, PACKED_YSIZE-y, zp);\n"
"#else\n"
"        real2 wfac = w[z];\n"
"        real2 z2 = loadComplexValue(in, xp, yp, PACKED_ZSIZE-z);\n"
"#endif\n"
"        real2 even = make_real2((z1.x+z2.x)/2, (z1.y-z2.y)/2);\n"
"        real2 odd = make_real2((z1.x-z2.x)/2, (z1.y+z2.y)/2);\n"
"        odd = make_real2(odd.x*wfac.x-odd.y*wfac.y, odd.y*wfac.x+odd.x*wfac.y);\n"
"        out[x*PACKED_YSIZE*PACKED_ZSIZE+y*PACKED_ZSIZE+z] = make_real2(even.x-odd.y, even.y+odd.x);\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Split the data back into a full sized, real grid after an inverse transform.\n"
" */\n"
"extern \"C\" __global__ void unpackBackwardData(const real2* __restrict__ in, real* __restrict__ out) {\n"
"    const int gridSize = PACKED_XSIZE*PACKED_YSIZE*PACKED_ZSIZE;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int x = index/(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int remainder = index-x*(PACKED_YSIZE*PACKED_ZSIZE);\n"
"        int y = remainder/PACKED_ZSIZE;\n"
"        int z = remainder-y*PACKED_ZSIZE;\n"
"        real2 value = 2*in[index];\n"
"#if PACKED_AXIS == 0\n"
"        out[2*x*YSIZE*ZSIZE+y*ZSIZE+z] = value.x;\n"
"        out[(2*x+1)*YSIZE*ZSIZE+y*ZSIZE+z] = value.y;\n"
"#elif PACKED_AXIS == 1\n"
"        out[x*YSIZE*ZSIZE+2*y*ZSIZE+z] = value.x;\n"
"        out[x*YSIZE*ZSIZE+(2*y+1)*ZSIZE+z] = value.y;\n"
"#else\n"
"        out[x*YSIZE*ZSIZE+y*ZSIZE+2*z] = value.x;\n"
"        out[x*YSIZE*ZSIZE+y*ZSIZE+(2*z+1)] = value.y;\n"
"#endif\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::findInteractingBlocks = "#define GROUP_SIZE 256\n"
"#define BUFFER_SIZE 256\n"
"\n"
"/**\n"
" * Find a bounding box for the atoms in each block.\n"
" */\n"
"extern \"C\" __global__ void findBlockBounds(int numAtoms, real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        const real4* __restrict__ posq, real4* __restrict__ blockCenter, real4* __restrict__ blockBoundingBox, int* __restrict__ rebuildNeighborList,\n"
"        real2* __restrict__ sortedBlocks) {\n"
"    int index = blockIdx.x*blockDim.x+threadIdx.x;\n"
"    int base = index*TILE_SIZE;\n"
"    while (base < numAtoms) {\n"
"        real4 pos = posq[base];\n"
"#ifdef USE_PERIODIC\n"
"        APPLY_PERIODIC_TO_POS(pos)\n"
"#endif\n"
"        real4 minPos = pos;\n"
"        real4 maxPos = pos;\n"
"        int last = min(base+TILE_SIZE, numAtoms);\n"
"        for (int i = base+1; i < last; i++) {\n"
"            pos = posq[i];\n"
"#ifdef USE_PERIODIC\n"
"            real4 center = 0.5f*(maxPos+minPos);\n"
"            APPLY_PERIODIC_TO_POS_WITH_CENTER(pos, center)\n"
"#endif\n"
"            minPos = make_real4(min(minPos.x,pos.x), min(minPos.y,pos.y), min(minPos.z,pos.z), 0);\n"
"            maxPos = make_real4(max(maxPos.x,pos.x), max(maxPos.y,pos.y), max(maxPos.z,pos.z), 0);\n"
"        }\n"
"        real4 blockSize = 0.5f*(maxPos-minPos);\n"
"        real4 center = 0.5f*(maxPos+minPos);\n"
"        center.w = 0;\n"
"        for (int i = base; i < last; i++) {\n"
"            pos = posq[i];\n"
"            real4 delta = posq[i]-center;\n"
"#ifdef USE_PERIODIC\n"
"            APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"            center.w = max(center.w, delta.x*delta.x+delta.y*delta.y+delta.z*delta.z);\n"
"        }\n"
"        center.w = sqrt(center.w);\n"
"        blockBoundingBox[index] = blockSize;\n"
"        blockCenter[index] = center;\n"
"        sortedBlocks[index] = make_real2(blockSize.x+blockSize.y+blockSize.z, index);\n"
"        index += blockDim.x*gridDim.x;\n"
"        base = index*TILE_SIZE;\n"
"    }\n"
"    if (blockIdx.x == 0 && threadIdx.x == 0)\n"
"        rebuildNeighborList[0] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Sort the data about bounding boxes so it can be accessed more efficiently in the next kernel.\n"
" */\n"
"extern \"C\" __global__ void sortBoxData(const real2* __restrict__ sortedBlock, const real4* __restrict__ blockCenter,\n"
"        const real4* __restrict__ blockBoundingBox, real4* __restrict__ sortedBlockCenter,\n"
"        real4* __restrict__ sortedBlockBoundingBox, const real4* __restrict__ posq, const real4* __restrict__ oldPositions,\n"
"        unsigned int* __restrict__ interactionCount, int* __restrict__ rebuildNeighborList, bool forceRebuild) {\n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_BLOCKS; i += blockDim.x*gridDim.x) {\n"
"        int index = (int) sortedBlock[i].y;\n"
"        sortedBlockCenter[i] = blockCenter[index];\n"
"        sortedBlockBoundingBox[i] = blockBoundingBox[index];\n"
"    }\n"
"    \n"
"    // Also check whether any atom has moved enough so that we really need to rebuild the neighbor list.\n"
"\n"
"    bool rebuild = forceRebuild;\n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x) {\n"
"        real4 delta = oldPositions[i]-posq[i];\n"
"        if (delta.x*delta.x + delta.y*delta.y + delta.z*delta.z > 0.25f*PADDING*PADDING)\n"
"            rebuild = true;\n"
"    }\n"
"    if (rebuild) {\n"
"        rebuildNeighborList[0] = 1;\n"
"        interactionCount[0] = 0;\n"
"        interactionCount[1] = 0;\n"
"    }\n"
"}\n"
"\n"
"__device__ int saveSinglePairs(int x, int* atoms, int* flags, int length, unsigned int maxSinglePairs, unsigned int* singlePairCount, int2* singlePairs, int* sumBuffer, volatile int& pairStartIndex) {\n"
"    // Record interactions that should be computed as single pairs rather than in blocks.\n"
"    \n"
"    const int indexInWarp = threadIdx.x%32;\n"
"    int sum = 0;\n"
"    for (int i = indexInWarp; i < length; i += 32) {\n"
"        int count = __popc(flags[i]);\n"
"        sum += (count <= MAX_BITS_FOR_PAIRS ? count : 0);\n"
"    }\n"
"    for (int i = 1; i < 32; i *= 2) {\n"
"        int n = __shfl_up_sync(0xffffffff, sum, i);\n"
"        if (indexInWarp >= i)\n"
"            sum += n;\n"
"    }\n"
"    if (indexInWarp == 31)\n"
"        pairStartIndex = atomicAdd(singlePairCount,(unsigned int) sum);\n"
"    __syncwarp();\n"
"    int prevSum = __shfl_up_sync(0xffffffff, sum, 1);\n"
"    int pairIndex = pairStartIndex + (indexInWarp > 0 ? prevSum : 0);\n"
"    for (int i = indexInWarp; i < length; i += 32) {\n"
"        int count = __popc(flags[i]);\n"
"        if (count <= MAX_BITS_FOR_PAIRS && pairIndex+count <= maxSinglePairs) {\n"
"            int f = flags[i];\n"
"            while (f != 0) {\n"
"                singlePairs[pairIndex] = make_int2(atoms[i], x*TILE_SIZE+__ffs(f)-1);\n"
"                f &= f-1;\n"
"                pairIndex++;\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    // Compact the remaining interactions.\n"
"    \n"
"    const int warpMask = (1<<indexInWarp)-1;\n"
"    int numCompacted = 0;\n"
"    for (int start = 0; start < length; start += 32) {\n"
"        int i = start+indexInWarp;\n"
"        int atom = atoms[i];\n"
"        int flag = flags[i];\n"
"        bool include = (i < length && __popc(flags[i]) > MAX_BITS_FOR_PAIRS);\n"
"        int includeFlags = BALLOT(include);\n"
"        if (include) {\n"
"            int index = numCompacted+__popc(includeFlags&warpMask);\n"
"            atoms[index] = atom;\n"
"            flags[index] = flag;\n"
"        }\n"
"        numCompacted += __popc(includeFlags);\n"
"    }\n"
"    return numCompacted;\n"
"}\n"
"\n"
"/**\n"
" * Compare the bounding boxes for each pair of atom blocks (comprised of 32 atoms each), forming a tile. If the two\n"
" * atom blocks are sufficiently far apart, mark them as non-interacting. There are two stages in the algorithm.\n"
" *\n"
" * STAGE 1:\n"
" *\n"
" * A coarse grained atom block against interacting atom block neighbour list is constructed. \n"
" *\n"
" * Each warp first loads in some block X of interest. Each thread within the warp then loads \n"
" * in a different atom block Y. If Y has exclusions with X, then Y is not processed.  If the bounding boxes \n"
" * of the two atom blocks are within the cutoff distance, then the two atom blocks are considered to be\n"
" * interacting and Y is added to the buffer for X.\n"
" *\n"
" * STAGE 2:\n"
" *\n"
" * A fine grained atom block against interacting atoms neighbour list is constructed.\n"
" *\n"
" * The warp loops over atom blocks Y that were found to (possibly) interact with atom block X.  Each thread\n"
" * in the warp loops over the 32 atoms in X and compares their positions to one particular atom from block Y.\n"
" * If it finds one closer than the cutoff distance, the atom is added to the list of atoms interacting with block X.\n"
" * This continues until the buffer fills up, at which point the results are written to global memory.\n"
" *\n"
" * [in] periodicBoxSize        - size of the rectangular periodic box\n"
" * [in] invPeriodicBoxSize     - inverse of the periodic box\n"
" * [in] blockCenter            - the center of each bounding box\n"
" * [in] blockBoundingBox       - bounding box of each atom block\n"
" * [out] interactionCount      - total number of tiles that have interactions\n"
" * [out] interactingTiles      - set of blocks that have interactions\n"
" * [out] interactingAtoms      - a list of atoms that interact with each atom block\n"
" * [in] posq                   - x,y,z coordinates of each atom and charge q\n"
" * [in] maxTiles               - maximum number of tiles to process, used for multi-GPUs\n"
" * [in] startBlockIndex        - first block to process, used for multi-GPUs,\n"
" * [in] numBlocks              - total number of atom blocks\n"
" * [in] sortedBlocks           - a sorted list of atom blocks based on volume\n"
" * [in] sortedBlockCenter      - sorted centers, duplicated for fast access to avoid indexing\n"
" * [in] sortedBlockBoundingBox - sorted bounding boxes, duplicated for fast access\n"
" * [in] exclusionIndices       - maps into exclusionRowIndices with the starting position for a given atom\n"
" * [in] exclusionRowIndices    - stores the a continuous list of exclusions\n"
" *           eg: block 0 is excluded from atom 3,5,6\n"
" *               block 1 is excluded from atom 3,4\n"
" *               block 2 is excluded from atom 1,3,5,6\n"
" *              exclusionIndices[0][3][5][8]\n"
" *           exclusionRowIndices[3][5][6][3][4][1][3][5][6]\n"
" *                         index 0  1  2  3  4  5  6  7  8 \n"
" * [out] oldPos                - stores the positions of the atoms in which this neighbourlist was built on\n"
" *                             - this is used to decide when to rebuild a neighbourlist\n"
" * [in] rebuildNeighbourList   - whether or not to execute this kernel\n"
" *\n"
" */\n"
"extern \"C\" __global__ void findBlocksWithInteractions(real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        unsigned int* __restrict__ interactionCount, int* __restrict__ interactingTiles, unsigned int* __restrict__ interactingAtoms,\n"
"        int2* __restrict__ singlePairs, const real4* __restrict__ posq, unsigned int maxTiles, unsigned int maxSinglePairs,\n"
"        unsigned int startBlockIndex, unsigned int numBlocks, real2* __restrict__ sortedBlocks, const real4* __restrict__ sortedBlockCenter,\n"
"        const real4* __restrict__ sortedBlockBoundingBox, const unsigned int* __restrict__ exclusionIndices, const unsigned int* __restrict__ exclusionRowIndices,\n"
"        real4* __restrict__ oldPositions, const int* __restrict__ rebuildNeighborList) {\n"
"\n"
"    if (rebuildNeighborList[0] == 0)\n"
"        return; // The neighbor list doesn't need to be rebuilt.\n"
"\n"
"    const int indexInWarp = threadIdx.x%32;\n"
"    const int warpStart = threadIdx.x-indexInWarp;\n"
"    const int totalWarps = blockDim.x*gridDim.x/32;\n"
"    const int warpIndex = (blockIdx.x*blockDim.x+threadIdx.x)/32;\n"
"    const int warpMask = (1<<indexInWarp)-1;\n"
"    __shared__ int workgroupBuffer[BUFFER_SIZE*(GROUP_SIZE/32)];\n"
"    __shared__ int workgroupFlagsBuffer[BUFFER_SIZE*(GROUP_SIZE/32)];\n"
"    __shared__ int warpExclusions[MAX_EXCLUSIONS*(GROUP_SIZE/32)];\n"
"    __shared__ real3 posBuffer[GROUP_SIZE];\n"
"    __shared__ volatile int workgroupTileIndex[GROUP_SIZE/32];\n"
"    __shared__ int worksgroupPairStartIndex[GROUP_SIZE/32];\n"
"    int* sumBuffer = (int*) posBuffer; // Reuse the same buffer to save memory\n"
"    int* buffer = workgroupBuffer+BUFFER_SIZE*(warpStart/32);\n"
"    int* flagsBuffer = workgroupFlagsBuffer+BUFFER_SIZE*(warpStart/32);\n"
"    int* exclusionsForX = warpExclusions+MAX_EXCLUSIONS*(warpStart/32);\n"
"    volatile int& tileStartIndex = workgroupTileIndex[warpStart/32];\n"
"    volatile int& pairStartIndex = worksgroupPairStartIndex[warpStart/32];\n"
"\n"
"    // Loop over blocks.\n"
"    \n"
"    for (int block1 = startBlockIndex+warpIndex; block1 < startBlockIndex+numBlocks; block1 += totalWarps) {\n"
"        // Load data for this block.  Note that all threads in a warp are processing the same block.\n"
"        \n"
"        real2 sortedKey = sortedBlocks[block1];\n"
"        int x = (int) sortedKey.y;\n"
"        real4 blockCenterX = sortedBlockCenter[block1];\n"
"        real4 blockSizeX = sortedBlockBoundingBox[block1];\n"
"        int neighborsInBuffer = 0;\n"
"        real3 pos1 = trimTo3(posq[x*TILE_SIZE+indexInWarp]);\n"
"#ifdef USE_PERIODIC\n"
"        const bool singlePeriodicCopy = (0.5f*periodicBoxSize.x-blockSizeX.x >= PADDED_CUTOFF &&\n"
"                                         0.5f*periodicBoxSize.y-blockSizeX.y >= PADDED_CUTOFF &&\n"
"                                         0.5f*periodicBoxSize.z-blockSizeX.z >= PADDED_CUTOFF);\n"
"        if (singlePeriodicCopy) {\n"
"            // The box is small enough that we can just translate all the atoms into a single periodic\n"
"            // box, then skip having to apply periodic boundary conditions later.\n"
"            \n"
"            APPLY_PERIODIC_TO_POS_WITH_CENTER(pos1, blockCenterX)\n"
"        }\n"
"#endif\n"
"        posBuffer[threadIdx.x] = pos1;\n"
"\n"
"        // Load exclusion data for block x.\n"
"        \n"
"        const int exclusionStart = exclusionRowIndices[x];\n"
"        const int exclusionEnd = exclusionRowIndices[x+1];\n"
"        const int numExclusions = exclusionEnd-exclusionStart;\n"
"        for (int j = indexInWarp; j < numExclusions; j += 32)\n"
"            exclusionsForX[j] = exclusionIndices[exclusionStart+j];\n"
"        if (MAX_EXCLUSIONS > 32)\n"
"            __syncthreads();\n"
"        \n"
"        // Loop over atom blocks to search for neighbors.  The threads in a warp compare block1 against 32\n"
"        // other blocks in parallel.\n"
"\n"
"        for (int block2Base = block1+1; block2Base < NUM_BLOCKS; block2Base += 32) {\n"
"            int block2 = block2Base+indexInWarp;\n"
"            bool includeBlock2 = (block2 < NUM_BLOCKS);\n"
"            bool forceInclude = false;\n"
"            if (includeBlock2) {\n"
"                real4 blockCenterY = sortedBlockCenter[block2];\n"
"                real4 blockSizeY = sortedBlockBoundingBox[block2];\n"
"                real4 blockDelta = blockCenterX-blockCenterY;\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(blockDelta)\n"
"#endif\n"
"                includeBlock2 &= (blockDelta.x*blockDelta.x+blockDelta.y*blockDelta.y+blockDelta.z*blockDelta.z < (PADDED_CUTOFF+blockCenterX.w+blockCenterY.w)*(PADDED_CUTOFF+blockCenterX.w+blockCenterY.w));\n"
"                blockDelta.x = max(0.0f, fabs(blockDelta.x)-blockSizeX.x-blockSizeY.x);\n"
"                blockDelta.y = max(0.0f, fabs(blockDelta.y)-blockSizeX.y-blockSizeY.y);\n"
"                blockDelta.z = max(0.0f, fabs(blockDelta.z)-blockSizeX.z-blockSizeY.z);\n"
"                includeBlock2 &= (blockDelta.x*blockDelta.x+blockDelta.y*blockDelta.y+blockDelta.z*blockDelta.z < PADDED_CUTOFF_SQUARED);\n"
"#ifdef TRICLINIC\n"
"                // The calculation to find the nearest periodic copy is only guaranteed to work if the nearest copy is less than half a box width away.\n"
"                // If there's any possibility we might have missed it, do a detailed check.\n"
"\n"
"                if (periodicBoxSize.z/2-blockSizeX.z-blockSizeY.z < PADDED_CUTOFF || periodicBoxSize.y/2-blockSizeX.y-blockSizeY.y < PADDED_CUTOFF)\n"
"                    includeBlock2 = forceInclude = true;\n"
"#endif\n"
"                if (includeBlock2) {\n"
"                    int y = (int) sortedBlocks[block2].y;\n"
"                    for (int k = 0; k < numExclusions; k++)\n"
"                        includeBlock2 &= (exclusionsForX[k] != y);\n"
"                }\n"
"            }\n"
"            \n"
"            // Loop over any blocks we identified as potentially containing neighbors.\n"
"            \n"
"            int includeBlockFlags = BALLOT(includeBlock2);\n"
"            int forceIncludeFlags = BALLOT(forceInclude);\n"
"            while (includeBlockFlags != 0) {\n"
"                int i = __ffs(includeBlockFlags)-1;\n"
"                includeBlockFlags &= includeBlockFlags-1;\n"
"                forceInclude = (forceIncludeFlags>>i) & 1;\n"
"                int y = (int) sortedBlocks[block2Base+i].y;\n"
"\n"
"                // Check each atom in block Y for interactions.\n"
"\n"
"                int atom2 = y*TILE_SIZE+indexInWarp;\n"
"                real3 pos2 = trimTo3(posq[atom2]);\n"
"#ifdef USE_PERIODIC\n"
"                if (singlePeriodicCopy) {\n"
"                    APPLY_PERIODIC_TO_POS_WITH_CENTER(pos2, blockCenterX)\n"
"                }\n"
"#endif\n"
"                real4 blockCenterY = sortedBlockCenter[block2Base+i];\n"
"                real3 atomDelta = posBuffer[warpStart+indexInWarp]-trimTo3(blockCenterY);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(atomDelta)\n"
"#endif\n"
"                int atomFlags = BALLOT(forceInclude || atomDelta.x*atomDelta.x+atomDelta.y*atomDelta.y+atomDelta.z*atomDelta.z < (PADDED_CUTOFF+blockCenterY.w)*(PADDED_CUTOFF+blockCenterY.w));\n"
"                int interacts = 0;\n"
"                if (atom2 < NUM_ATOMS && atomFlags != 0) {\n"
"                    int first = __ffs(atomFlags)-1;\n"
"                    int last = 32-__clz(atomFlags);\n"
"#ifdef USE_PERIODIC\n"
"                    if (!singlePeriodicCopy) {\n"
"                        for (int j = first; j < last; j++) {\n"
"                            real3 delta = pos2-posBuffer[warpStart+j];\n"
"                            APPLY_PERIODIC_TO_DELTA(delta)\n"
"                            interacts |= (delta.x*delta.x+delta.y*delta.y+delta.z*delta.z < PADDED_CUTOFF_SQUARED ? 1<<j : 0);\n"
"                        }\n"
"                    }\n"
"                    else {\n"
"#endif\n"
"                        for (int j = first; j < last; j++) {\n"
"                            real3 delta = pos2-posBuffer[warpStart+j];\n"
"                            interacts |= (delta.x*delta.x+delta.y*delta.y+delta.z*delta.z < PADDED_CUTOFF_SQUARED ? 1<<j : 0);\n"
"                        }\n"
"#ifdef USE_PERIODIC\n"
"                    }\n"
"#endif\n"
"                }\n"
"                \n"
"                // Add any interacting atoms to the buffer.\n"
"                \n"
"                int includeAtomFlags = BALLOT(interacts);\n"
"                if (interacts) {\n"
"                    int index = neighborsInBuffer+__popc(includeAtomFlags&warpMask);\n"
"                    buffer[index] = atom2;\n"
"                    flagsBuffer[index] = interacts;\n"
"                }\n"
"                neighborsInBuffer += __popc(includeAtomFlags);\n"
"                if (neighborsInBuffer > BUFFER_SIZE-TILE_SIZE) {\n"
"                    // Store the new tiles to memory.\n"
"                    \n"
"#if MAX_BITS_FOR_PAIRS > 0\n"
"                    neighborsInBuffer = saveSinglePairs(x, buffer, flagsBuffer, neighborsInBuffer, maxSinglePairs, &interactionCount[1], singlePairs, sumBuffer+warpStart, pairStartIndex);\n"
"#endif\n"
"                    int tilesToStore = neighborsInBuffer/TILE_SIZE;\n"
"                    if (tilesToStore > 0) {\n"
"                        if (indexInWarp == 0)\n"
"                            tileStartIndex = atomicAdd(&interactionCount[0], tilesToStore);\n"
"                        int newTileStartIndex = tileStartIndex;\n"
"                        if (newTileStartIndex+tilesToStore <= maxTiles) {\n"
"                            if (indexInWarp < tilesToStore)\n"
"                                interactingTiles[newTileStartIndex+indexInWarp] = x;\n"
"                            for (int j = 0; j < tilesToStore; j++)\n"
"                                interactingAtoms[(newTileStartIndex+j)*TILE_SIZE+indexInWarp] = buffer[indexInWarp+j*TILE_SIZE];\n"
"                        }\n"
"                        if (indexInWarp+TILE_SIZE*tilesToStore < BUFFER_SIZE)\n"
"                            buffer[indexInWarp] = buffer[indexInWarp+TILE_SIZE*tilesToStore];\n"
"                        neighborsInBuffer -= TILE_SIZE*tilesToStore;\n"
"                    }\n"
"                }\n"
"            }\n"
"        }\n"
"        \n"
"        // If we have a partially filled buffer,  store it to memory.\n"
"        \n"
"#if MAX_BITS_FOR_PAIRS > 0\n"
"        if (neighborsInBuffer > 32)\n"
"            neighborsInBuffer = saveSinglePairs(x, buffer, flagsBuffer, neighborsInBuffer, maxSinglePairs, &interactionCount[1], singlePairs, sumBuffer+warpStart, pairStartIndex);\n"
"#endif\n"
"        if (neighborsInBuffer > 0) {\n"
"            int tilesToStore = (neighborsInBuffer+TILE_SIZE-1)/TILE_SIZE;\n"
"            if (indexInWarp == 0)\n"
"                tileStartIndex = atomicAdd(&interactionCount[0], tilesToStore);\n"
"            int newTileStartIndex = tileStartIndex;\n"
"            if (newTileStartIndex+tilesToStore <= maxTiles) {\n"
"                if (indexInWarp < tilesToStore)\n"
"                    interactingTiles[newTileStartIndex+indexInWarp] = x;\n"
"                for (int j = 0; j < tilesToStore; j++)\n"
"                    interactingAtoms[(newTileStartIndex+j)*TILE_SIZE+indexInWarp] = (indexInWarp+j*TILE_SIZE < neighborsInBuffer ? buffer[indexInWarp+j*TILE_SIZE] : NUM_ATOMS);\n"
"            }\n"
"        }\n"
"    }\n"
"    \n"
"    // Record the positions the neighbor list is based on.\n"
"    \n"
"    for (int i = threadIdx.x+blockIdx.x*blockDim.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x)\n"
"        oldPositions[i] = posq[i];\n"
"}\n"
"";
const string CudaKernelSources::monteCarloBarostat = "/**\n"
" * Scale the particle positions with each axis independent\n"
" */\n"
"\n"
"extern \"C\" __global__ void scalePositions(float scaleX, float scaleY, float scaleZ, int numMolecules, real4 periodicBoxSize,\n"
"        real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ, real4* __restrict__ posq,\n"
"        const int* __restrict__ moleculeAtoms, const int* __restrict__ moleculeStartIndex) {\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < numMolecules; index += blockDim.x*gridDim.x) {\n"
"        int first = moleculeStartIndex[index];\n"
"        int last = moleculeStartIndex[index+1];\n"
"        int numAtoms = last-first;\n"
"\n"
"        // Find the center of each molecule.\n"
"\n"
"        real3 center = make_real3(0, 0, 0);\n"
"        for (int atom = first; atom < last; atom++) {\n"
"            real4 pos = posq[moleculeAtoms[atom]];\n"
"            center.x += pos.x;\n"
"            center.y += pos.y;\n"
"            center.z += pos.z;\n"
"        }\n"
"        real invNumAtoms = RECIP(numAtoms);\n"
"        center.x *= invNumAtoms;\n"
"        center.y *= invNumAtoms;\n"
"        center.z *= invNumAtoms;\n"
"\n"
"        // Move it into the first periodic box.\n"
"\n"
"        real3 oldCenter = center;\n"
"        APPLY_PERIODIC_TO_POS(center)\n"
"        real3 delta = make_real3(oldCenter.x-center.x, oldCenter.y-center.y, oldCenter.z-center.z);\n"
"\n"
"        // Now scale the position of the molecule center.\n"
"\n"
"        delta.x = center.x*(scaleX-1)-delta.x;\n"
"        delta.y = center.y*(scaleY-1)-delta.y;\n"
"        delta.z = center.z*(scaleZ-1)-delta.z;\n"
"        for (int atom = first; atom < last; atom++) {\n"
"            real4 pos = posq[moleculeAtoms[atom]];\n"
"            pos.x += delta.x;\n"
"            pos.y += delta.y;\n"
"            pos.z += delta.z;\n"
"            posq[moleculeAtoms[atom]] = pos;\n"
"        }\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::nonbonded = "#define WARPS_PER_GROUP (THREAD_BLOCK_SIZE/TILE_SIZE)\n"
"\n"
"#ifndef ENABLE_SHUFFLE\n"
"typedef struct {\n"
"    real x, y, z;\n"
"    real q;\n"
"    real fx, fy, fz;\n"
"    ATOM_PARAMETER_DATA\n"
"#ifndef PARAMETER_SIZE_IS_EVEN\n"
"    real padding;\n"
"#endif\n"
"} AtomData;\n"
"#endif\n"
"\n"
"#ifdef ENABLE_SHUFFLE\n"
"//support for 64 bit shuffles\n"
"static __inline__ __device__ float real_shfl(float var, int srcLane) {\n"
"    return SHFL(var, srcLane);\n"
"}\n"
"\n"
"static __inline__ __device__ float real_shfl(int var, int srcLane) {\n"
"    return SHFL(var, srcLane);\n"
"}\n"
"\n"
"static __inline__ __device__ double real_shfl(double var, int srcLane) {\n"
"    int hi, lo;\n"
"    asm volatile(\"mov.b64 { %0, %1 }, %2;\" : \"=r\"(lo), \"=r\"(hi) : \"d\"(var));\n"
"    hi = SHFL(hi, srcLane);\n"
"    lo = SHFL(lo, srcLane);\n"
"    return __hiloint2double( hi, lo );\n"
"}\n"
"\n"
"static __inline__ __device__ long long real_shfl(long long var, int srcLane) {\n"
"    int hi, lo;\n"
"    asm volatile(\"mov.b64 { %0, %1 }, %2;\" : \"=r\"(lo), \"=r\"(hi) : \"l\"(var));\n"
"    hi = SHFL(hi, srcLane);\n"
"    lo = SHFL(lo, srcLane);\n"
"    // unforunately there isn't an __nv_hiloint2long(hi,lo) intrinsic cast\n"
"    int2 fuse; fuse.x = lo; fuse.y = hi;\n"
"    return *reinterpret_cast<long long*>(&fuse);\n"
"}\n"
"#endif\n"
"\n"
"/**\n"
" * Compute nonbonded interactions. The kernel is separated into two parts,\n"
" * tiles with exclusions and tiles without exclusions. It relies heavily on \n"
" * implicit warp-level synchronization. A tile is defined by two atom blocks \n"
" * each of warpsize. Each warp computes a range of tiles.\n"
" * \n"
" * Tiles with exclusions compute the entire set of interactions across\n"
" * atom blocks, equal to warpsize*warpsize. In order to avoid access conflicts \n"
" * the forces are computed and accumulated diagonally in the manner shown below\n"
" * where, suppose\n"
" *\n"
" * [a-h] comprise atom block 1, [i-p] comprise atom block 2\n"
" *\n"
" * 1 denotes the first set of calculations within the warp\n"
" * 2 denotes the second set of calculations within the warp\n"
" * ... etc.\n"
" * \n"
" *        threads\n"
" *     0 1 2 3 4 5 6 7\n"
" *         atom1 \n"
" * L    a b c d e f g h \n"
" * o  i 1 2 3 4 5 6 7 8\n"
" * c  j 8 1 2 3 4 5 6 7\n"
" * a  k 7 8 1 2 3 4 5 6\n"
" * l  l 6 7 8 1 2 3 4 5\n"
" * D  m 5 6 7 8 1 2 3 4 \n"
" * a  n 4 5 6 7 8 1 2 3\n"
" * t  o 3 4 5 6 7 8 1 2\n"
" * a  p 2 3 4 5 6 7 8 1\n"
" *\n"
" * Tiles without exclusions read off directly from the neighbourlist interactingAtoms\n"
" * and follows the same force accumulation method. If more there are more interactingTiles\n"
" * than the size of the neighbourlist initially allocated, the neighbourlist is rebuilt\n"
" * and the full tileset is computed. This should happen on the first step, and very rarely \n"
" * afterwards.\n"
" *\n"
" * On CUDA devices that support the shuffle intrinsic, on diagonal exclusion tiles use\n"
" * __shfl to broadcast. For all other types of tiles __shfl is used to pass around the \n"
" * forces, positions, and parameters when computing the forces. \n"
" *\n"
" * [out]forceBuffers    - forces on each atom to eventually be accumulated\n"
" * [out]energyBuffer    - energyBuffer to eventually be accumulated\n"
" * [in]posq             - x,y,z,charge \n"
" * [in]exclusions       - 1024-bit flags denoting atom-atom exclusions for each tile\n"
" * [in]exclusionTiles   - x,y denotes the indices of tiles that have an exclusion\n"
" * [in]startTileIndex   - index into first tile to be processed\n"
" * [in]numTileIndices   - number of tiles this context is responsible for processing\n"
" * [in]int tiles        - the atom block for each tile\n"
" * [in]interactionCount - total number of tiles that have an interaction\n"
" * [in]maxTiles         - stores the size of the neighbourlist in case it needs \n"
" *                      - to be expanded\n"
" * [in]periodicBoxSize  - size of the Periodic Box, last dimension (w) not used\n"
" * [in]invPeriodicBox   - inverse of the periodicBoxSize, pre-computed for speed\n"
" * [in]blockCenter      - the center of each block in euclidean coordinates\n"
" * [in]blockSize        - size of the each block, radiating from the center\n"
" *                      - x is half the distance of total length\n"
" *                      - y is half the distance of total width\n"
" *                      - z is half the distance of total height\n"
" *                      - w is not used\n"
" * [in]interactingAtoms - a list of interactions within a given tile     \n"
" *\n"
" */\n"
"extern \"C\" __global__ void computeNonbonded(\n"
"        unsigned long long* __restrict__ forceBuffers, mixed* __restrict__ energyBuffer, const real4* __restrict__ posq, const tileflags* __restrict__ exclusions,\n"
"        const int2* __restrict__ exclusionTiles, unsigned int startTileIndex, unsigned long long numTileIndices\n"
"#ifdef USE_CUTOFF\n"
"        , const int* __restrict__ tiles, const unsigned int* __restrict__ interactionCount, real4 periodicBoxSize, real4 invPeriodicBoxSize, \n"
"        real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ, unsigned int maxTiles, const real4* __restrict__ blockCenter,\n"
"        const real4* __restrict__ blockSize, const unsigned int* __restrict__ interactingAtoms, unsigned int maxSinglePairs,\n"
"        const int2* __restrict__ singlePairs\n"
"#endif\n"
"        PARAMETER_ARGUMENTS) {\n"
"    const unsigned int totalWarps = (blockDim.x*gridDim.x)/TILE_SIZE;\n"
"    const unsigned int warp = (blockIdx.x*blockDim.x+threadIdx.x)/TILE_SIZE; // global warpIndex\n"
"    const unsigned int tgx = threadIdx.x & (TILE_SIZE-1); // index within the warp\n"
"    const unsigned int tbx = threadIdx.x - tgx;           // block warpIndex\n"
"    mixed energy = 0;\n"
"    INIT_DERIVATIVES\n"
"    // used shared memory if the device cannot shuffle\n"
"#ifndef ENABLE_SHUFFLE\n"
"    __shared__ AtomData localData[THREAD_BLOCK_SIZE];\n"
"#endif\n"
"\n"
"    // First loop: process tiles that contain exclusions.\n"
"\n"
"    const unsigned int firstExclusionTile = FIRST_EXCLUSION_TILE+warp*(LAST_EXCLUSION_TILE-FIRST_EXCLUSION_TILE)/totalWarps;\n"
"    const unsigned int lastExclusionTile = FIRST_EXCLUSION_TILE+(warp+1)*(LAST_EXCLUSION_TILE-FIRST_EXCLUSION_TILE)/totalWarps;\n"
"    for (int pos = firstExclusionTile; pos < lastExclusionTile; pos++) {\n"
"        const int2 tileIndices = exclusionTiles[pos];\n"
"        const unsigned int x = tileIndices.x;\n"
"        const unsigned int y = tileIndices.y;\n"
"        real3 force = make_real3(0);\n"
"        unsigned int atom1 = x*TILE_SIZE + tgx;\n"
"        real4 posq1 = posq[atom1];\n"
"        LOAD_ATOM1_PARAMETERS\n"
"#ifdef USE_EXCLUSIONS\n"
"        tileflags excl = exclusions[pos*TILE_SIZE+tgx];\n"
"#endif\n"
"        const bool hasExclusions = true;\n"
"        if (x == y) {\n"
"            // This tile is on the diagonal.\n"
"#ifdef ENABLE_SHUFFLE\n"
"            real4 shflPosq = posq1;\n"
"#else\n"
"            localData[threadIdx.x].x = posq1.x;\n"
"            localData[threadIdx.x].y = posq1.y;\n"
"            localData[threadIdx.x].z = posq1.z;\n"
"            localData[threadIdx.x].q = posq1.w;\n"
"            LOAD_LOCAL_PARAMETERS_FROM_1\n"
"#endif\n"
"\n"
"            // we do not need to fetch parameters from global since this is a symmetric tile\n"
"            // instead we can broadcast the values using shuffle\n"
"            for (unsigned int j = 0; j < TILE_SIZE; j++) {\n"
"                int atom2 = tbx+j;\n"
"                real4 posq2;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                BROADCAST_WARP_DATA\n"
"#else   \n"
"                posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                real invR = RSQRT(r2);\n"
"                real r = r2*invR;\n"
"                LOAD_ATOM2_PARAMETERS\n"
"                atom2 = y*TILE_SIZE+j;\n"
"#ifdef USE_SYMMETRIC\n"
"                real dEdR = 0.0f;\n"
"#else\n"
"                real3 dEdR1 = make_real3(0);\n"
"                real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS || !(excl & 0x1));\n"
"#endif\n"
"                real tempEnergy = 0.0f;\n"
"                const real interactionScale = 0.5f;\n"
"                COMPUTE_INTERACTION\n"
"                energy += 0.5f*tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                force.x -= delta.x*dEdR;\n"
"                force.y -= delta.y*dEdR;\n"
"                force.z -= delta.z*dEdR;\n"
"#else\n"
"                force.x -= dEdR1.x;\n"
"                force.y -= dEdR1.y;\n"
"                force.z -= dEdR1.z;\n"
"#endif\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                excl >>= 1;\n"
"#endif\n"
"            }\n"
"        }\n"
"        else {\n"
"            // This is an off-diagonal tile.\n"
"            unsigned int j = y*TILE_SIZE + tgx;\n"
"            real4 shflPosq = posq[j];\n"
"#ifdef ENABLE_SHUFFLE\n"
"            real3 shflForce;\n"
"            shflForce.x = 0.0f;\n"
"            shflForce.y = 0.0f;\n"
"            shflForce.z = 0.0f;\n"
"#else\n"
"            localData[threadIdx.x].x = shflPosq.x;\n"
"            localData[threadIdx.x].y = shflPosq.y;\n"
"            localData[threadIdx.x].z = shflPosq.z;\n"
"            localData[threadIdx.x].q = shflPosq.w;\n"
"            localData[threadIdx.x].fx = 0.0f;\n"
"            localData[threadIdx.x].fy = 0.0f;\n"
"            localData[threadIdx.x].fz = 0.0f;\n"
"#endif\n"
"            DECLARE_LOCAL_PARAMETERS\n"
"            LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"#ifdef USE_EXCLUSIONS\n"
"            excl = (excl >> tgx) | (excl << (TILE_SIZE - tgx));\n"
"#endif\n"
"            unsigned int tj = tgx;\n"
"            for (j = 0; j < TILE_SIZE; j++) {\n"
"                int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                real4 posq2 = shflPosq;\n"
"#else\n"
"                real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                real invR = RSQRT(r2);\n"
"                real r = r2*invR;\n"
"                LOAD_ATOM2_PARAMETERS\n"
"                atom2 = y*TILE_SIZE+tj;\n"
"#ifdef USE_SYMMETRIC\n"
"                real dEdR = 0.0f;\n"
"#else\n"
"                real3 dEdR1 = make_real3(0);\n"
"                real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS || !(excl & 0x1));\n"
"#endif\n"
"                real tempEnergy = 0.0f;\n"
"                const real interactionScale = 1.0f;\n"
"                COMPUTE_INTERACTION\n"
"                energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                delta *= dEdR;\n"
"                force.x -= delta.x;\n"
"                force.y -= delta.y;\n"
"                force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflForce.x += delta.x;\n"
"                shflForce.y += delta.y;\n"
"                shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                localData[tbx+tj].fx += delta.x;\n"
"                localData[tbx+tj].fy += delta.y;\n"
"                localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                force.x -= dEdR1.x;\n"
"                force.y -= dEdR1.y;\n"
"                force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflForce.x += dEdR2.x;\n"
"                shflForce.y += dEdR2.y;\n"
"                shflForce.z += dEdR2.z;\n"
"#else\n"
"                localData[tbx+tj].fx += dEdR2.x;\n"
"                localData[tbx+tj].fy += dEdR2.y;\n"
"                localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                SHUFFLE_WARP_DATA\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                excl >>= 1;\n"
"#endif\n"
"                // cycles the indices\n"
"                // 0 1 2 3 4 5 6 7 -> 1 2 3 4 5 6 7 0\n"
"                tj = (tj + 1) & (TILE_SIZE - 1);\n"
"            }\n"
"            const unsigned int offset = y*TILE_SIZE + tgx;\n"
"            // write results for off diagonal tiles\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef ENABLE_SHUFFLE\n"
"            atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (shflForce.x*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.y*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.z*0x100000000)));\n"
"#else\n"
"            atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fx*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fy*0x100000000)));\n"
"            atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fz*0x100000000)));\n"
"#endif\n"
"#endif\n"
"        }\n"
"        // Write results for on and off diagonal tiles\n"
"#ifdef INCLUDE_FORCES\n"
"        const unsigned int offset = x*TILE_SIZE + tgx;\n"
"        atomicAdd(&forceBuffers[offset], static_cast<unsigned long long>((long long) (force.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[offset+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[offset+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.z*0x100000000)));\n"
"#endif\n"
"    }\n"
"\n"
"    // Second loop: tiles without exclusions, either from the neighbor list (with cutoff) or just enumerating all\n"
"    // of them (no cutoff).\n"
"\n"
"#ifdef USE_CUTOFF\n"
"    const unsigned int numTiles = interactionCount[0];\n"
"    if (numTiles > maxTiles)\n"
"        return; // There wasn't enough memory for the neighbor list.\n"
"    int pos = (int) (warp*(long long)numTiles/totalWarps);\n"
"    int end = (int) ((warp+1)*(long long)numTiles/totalWarps);\n"
"#else\n"
"    int pos = (int) (startTileIndex+warp*numTileIndices/totalWarps);\n"
"    int end = (int) (startTileIndex+(warp+1)*numTileIndices/totalWarps);\n"
"#endif\n"
"    int skipBase = 0;\n"
"    int currentSkipIndex = tbx;\n"
"    // atomIndices can probably be shuffled as well\n"
"    // but it probably wouldn't make things any faster\n"
"    __shared__ int atomIndices[THREAD_BLOCK_SIZE];\n"
"    __shared__ volatile int skipTiles[THREAD_BLOCK_SIZE];\n"
"    skipTiles[threadIdx.x] = -1;\n"
"    \n"
"    while (pos < end) {\n"
"        const bool hasExclusions = false;\n"
"        real3 force = make_real3(0);\n"
"        bool includeTile = true;\n"
"\n"
"        // Extract the coordinates of this tile.\n"
"        int x, y;\n"
"        bool singlePeriodicCopy = false;\n"
"#ifdef USE_CUTOFF\n"
"        x = tiles[pos];\n"
"        real4 blockSizeX = blockSize[x];\n"
"        singlePeriodicCopy = (0.5f*periodicBoxSize.x-blockSizeX.x >= MAX_CUTOFF &&\n"
"                              0.5f*periodicBoxSize.y-blockSizeX.y >= MAX_CUTOFF &&\n"
"                              0.5f*periodicBoxSize.z-blockSizeX.z >= MAX_CUTOFF);\n"
"#else\n"
"        y = (int) floor(NUM_BLOCKS+0.5f-SQRT((NUM_BLOCKS+0.5f)*(NUM_BLOCKS+0.5f)-2*pos));\n"
"        x = (pos-y*NUM_BLOCKS+y*(y+1)/2);\n"
"        if (x < y || x >= NUM_BLOCKS) { // Occasionally happens due to roundoff error.\n"
"            y += (x < y ? -1 : 1);\n"
"            x = (pos-y*NUM_BLOCKS+y*(y+1)/2);\n"
"        }\n"
"\n"
"        // Skip over tiles that have exclusions, since they were already processed.\n"
"\n"
"        while (skipTiles[tbx+TILE_SIZE-1] < pos) {\n"
"            if (skipBase+tgx < NUM_TILES_WITH_EXCLUSIONS) {\n"
"                int2 tile = exclusionTiles[skipBase+tgx];\n"
"                skipTiles[threadIdx.x] = tile.x + tile.y*NUM_BLOCKS - tile.y*(tile.y+1)/2;\n"
"            }\n"
"            else\n"
"                skipTiles[threadIdx.x] = end;\n"
"            skipBase += TILE_SIZE;            \n"
"            currentSkipIndex = tbx;\n"
"        }\n"
"        while (skipTiles[currentSkipIndex] < pos)\n"
"            currentSkipIndex++;\n"
"        includeTile = (skipTiles[currentSkipIndex] != pos);\n"
"#endif\n"
"        if (includeTile) {\n"
"            unsigned int atom1 = x*TILE_SIZE + tgx;\n"
"            // Load atom data for this tile.\n"
"            real4 posq1 = posq[atom1];\n"
"            LOAD_ATOM1_PARAMETERS\n"
"            //const unsigned int localAtomIndex = threadIdx.x;\n"
"#ifdef USE_CUTOFF\n"
"            unsigned int j = interactingAtoms[pos*TILE_SIZE+tgx];\n"
"#else\n"
"            unsigned int j = y*TILE_SIZE + tgx;\n"
"#endif\n"
"            atomIndices[threadIdx.x] = j;\n"
"#ifdef ENABLE_SHUFFLE\n"
"            DECLARE_LOCAL_PARAMETERS\n"
"            real4 shflPosq;\n"
"            real3 shflForce;\n"
"            shflForce.x = 0.0f;\n"
"            shflForce.y = 0.0f;\n"
"            shflForce.z = 0.0f;\n"
"#endif\n"
"            if (j < PADDED_NUM_ATOMS) {\n"
"                // Load position of atom j from from global memory\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflPosq = posq[j];\n"
"#else\n"
"                localData[threadIdx.x].x = posq[j].x;\n"
"                localData[threadIdx.x].y = posq[j].y;\n"
"                localData[threadIdx.x].z = posq[j].z;\n"
"                localData[threadIdx.x].q = posq[j].w;\n"
"                localData[threadIdx.x].fx = 0.0f;\n"
"                localData[threadIdx.x].fy = 0.0f;\n"
"                localData[threadIdx.x].fz = 0.0f;\n"
"#endif                \n"
"                LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"            }\n"
"            else {\n"
"#ifdef ENABLE_SHUFFLE\n"
"                shflPosq = make_real4(0, 0, 0, 0);\n"
"#else\n"
"                localData[threadIdx.x].x = 0;\n"
"                localData[threadIdx.x].y = 0;\n"
"                localData[threadIdx.x].z = 0;\n"
"#endif\n"
"                CLEAR_LOCAL_PARAMETERS\n"
"            }\n"
"#ifdef USE_PERIODIC\n"
"            if (singlePeriodicCopy) {\n"
"                // The box is small enough that we can just translate all the atoms into a single periodic\n"
"                // box, then skip having to apply periodic boundary conditions later.\n"
"                real4 blockCenterX = blockCenter[x];\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(posq1, blockCenterX)\n"
"#ifdef ENABLE_SHUFFLE\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(shflPosq, blockCenterX)\n"
"#else\n"
"                APPLY_PERIODIC_TO_POS_WITH_CENTER(localData[threadIdx.x], blockCenterX)\n"
"#endif\n"
"                unsigned int tj = tgx;\n"
"                for (j = 0; j < TILE_SIZE; j++) {\n"
"                    int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    real4 posq2 = shflPosq; \n"
"#else\n"
"                    real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                    real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"                    real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                    real invR = RSQRT(r2);\n"
"                    real r = r2*invR;\n"
"                    LOAD_ATOM2_PARAMETERS\n"
"                    atom2 = atomIndices[tbx+tj];\n"
"#ifdef USE_SYMMETRIC\n"
"                    real dEdR = 0.0f;\n"
"#else\n"
"                    real3 dEdR1 = make_real3(0);\n"
"                    real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                    bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS);\n"
"#endif\n"
"                    real tempEnergy = 0.0f;\n"
"                    const real interactionScale = 1.0f;\n"
"                    COMPUTE_INTERACTION\n"
"                    energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                    delta *= dEdR;\n"
"                    force.x -= delta.x;\n"
"                    force.y -= delta.y;\n"
"                    force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += delta.x;\n"
"                    shflForce.y += delta.y;\n"
"                    shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                    localData[tbx+tj].fx += delta.x;\n"
"                    localData[tbx+tj].fy += delta.y;\n"
"                    localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                    force.x -= dEdR1.x;\n"
"                    force.y -= dEdR1.y;\n"
"                    force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += dEdR2.x;\n"
"                    shflForce.y += dEdR2.y;\n"
"                    shflForce.z += dEdR2.z;\n"
"#else\n"
"                    localData[tbx+tj].fx += dEdR2.x;\n"
"                    localData[tbx+tj].fy += dEdR2.y;\n"
"                    localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    SHUFFLE_WARP_DATA\n"
"#endif\n"
"                    tj = (tj + 1) & (TILE_SIZE - 1);\n"
"                }\n"
"            }\n"
"            else\n"
"#endif\n"
"            {\n"
"                // We need to apply periodic boundary conditions separately for each interaction.\n"
"                unsigned int tj = tgx;\n"
"                for (j = 0; j < TILE_SIZE; j++) {\n"
"                    int atom2 = tbx+tj;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    real4 posq2 = shflPosq;\n"
"#else\n"
"                    real4 posq2 = make_real4(localData[atom2].x, localData[atom2].y, localData[atom2].z, localData[atom2].q);\n"
"#endif\n"
"                    real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"                    APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"                    real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"                    real invR = RSQRT(r2);\n"
"                    real r = r2*invR;\n"
"                    LOAD_ATOM2_PARAMETERS\n"
"                    atom2 = atomIndices[tbx+tj];\n"
"#ifdef USE_SYMMETRIC\n"
"                    real dEdR = 0.0f;\n"
"#else\n"
"                    real3 dEdR1 = make_real3(0);\n"
"                    real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"#ifdef USE_EXCLUSIONS\n"
"                    bool isExcluded = (atom1 >= NUM_ATOMS || atom2 >= NUM_ATOMS);\n"
"#endif\n"
"                    real tempEnergy = 0.0f;\n"
"                    const real interactionScale = 1.0f;\n"
"                    COMPUTE_INTERACTION\n"
"                    energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"                    delta *= dEdR;\n"
"                    force.x -= delta.x;\n"
"                    force.y -= delta.y;\n"
"                    force.z -= delta.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += delta.x;\n"
"                    shflForce.y += delta.y;\n"
"                    shflForce.z += delta.z;\n"
"\n"
"#else\n"
"                    localData[tbx+tj].fx += delta.x;\n"
"                    localData[tbx+tj].fy += delta.y;\n"
"                    localData[tbx+tj].fz += delta.z;\n"
"#endif\n"
"#else // !USE_SYMMETRIC\n"
"                    force.x -= dEdR1.x;\n"
"                    force.y -= dEdR1.y;\n"
"                    force.z -= dEdR1.z;\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    shflForce.x += dEdR2.x;\n"
"                    shflForce.y += dEdR2.y;\n"
"                    shflForce.z += dEdR2.z;\n"
"#else\n"
"                    localData[tbx+tj].fx += dEdR2.x;\n"
"                    localData[tbx+tj].fy += dEdR2.y;\n"
"                    localData[tbx+tj].fz += dEdR2.z;\n"
"#endif \n"
"#endif // end USE_SYMMETRIC\n"
"#endif\n"
"#ifdef ENABLE_SHUFFLE\n"
"                    SHUFFLE_WARP_DATA\n"
"#endif\n"
"                    tj = (tj + 1) & (TILE_SIZE - 1);\n"
"                }\n"
"            }\n"
"\n"
"            // Write results.\n"
"#ifdef INCLUDE_FORCES\n"
"            atomicAdd(&forceBuffers[atom1], static_cast<unsigned long long>((long long) (force.x*0x100000000)));\n"
"            atomicAdd(&forceBuffers[atom1+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.y*0x100000000)));\n"
"            atomicAdd(&forceBuffers[atom1+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (force.z*0x100000000)));\n"
"#ifdef USE_CUTOFF\n"
"            unsigned int atom2 = atomIndices[threadIdx.x];\n"
"#else\n"
"            unsigned int atom2 = y*TILE_SIZE + tgx;\n"
"#endif\n"
"            if (atom2 < PADDED_NUM_ATOMS) {\n"
"#ifdef ENABLE_SHUFFLE\n"
"                atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (shflForce.x*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.y*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (shflForce.z*0x100000000)));\n"
"#else\n"
"                atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fx*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fy*0x100000000)));\n"
"                atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (localData[threadIdx.x].fz*0x100000000)));\n"
"#endif\n"
"            }\n"
"#endif\n"
"        }\n"
"        pos++;\n"
"    }\n"
"    \n"
"    // Third loop: single pairs that aren't part of a tile.\n"
"    \n"
"#if USE_CUTOFF\n"
"    const unsigned int numPairs = interactionCount[1];\n"
"    if (numPairs > maxSinglePairs)\n"
"        return; // There wasn't enough memory for the neighbor list.\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numPairs; i += blockDim.x*gridDim.x) {\n"
"        int2 pair = singlePairs[i];\n"
"        int atom1 = pair.x;\n"
"        int atom2 = pair.y;\n"
"        real4 posq1 = posq[atom1];\n"
"        real4 posq2 = posq[atom2];\n"
"        LOAD_ATOM1_PARAMETERS\n"
"        int j = atom2;\n"
"        atom2 = threadIdx.x;\n"
"        DECLARE_LOCAL_PARAMETERS\n"
"        LOAD_LOCAL_PARAMETERS_FROM_GLOBAL\n"
"        LOAD_ATOM2_PARAMETERS\n"
"        atom2 = pair.y;\n"
"        real3 delta = make_real3(posq2.x-posq1.x, posq2.y-posq1.y, posq2.z-posq1.z);\n"
"#ifdef USE_PERIODIC\n"
"        APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"        real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"        real invR = RSQRT(r2);\n"
"        real r = r2*invR;\n"
"#ifdef USE_SYMMETRIC\n"
"        real dEdR = 0.0f;\n"
"#else\n"
"        real3 dEdR1 = make_real3(0);\n"
"        real3 dEdR2 = make_real3(0);\n"
"#endif\n"
"        bool hasExclusions = false;\n"
"        bool isExcluded = false;\n"
"        real tempEnergy = 0.0f;\n"
"        const real interactionScale = 1.0f;\n"
"        COMPUTE_INTERACTION\n"
"        energy += tempEnergy;\n"
"#ifdef INCLUDE_FORCES\n"
"#ifdef USE_SYMMETRIC\n"
"        real3 dEdR1 = delta*dEdR;\n"
"        real3 dEdR2 = -dEdR1;\n"
"#endif\n"
"        atomicAdd(&forceBuffers[atom1], static_cast<unsigned long long>((long long) (-dEdR1.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom1+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR1.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom1+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR1.z*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2], static_cast<unsigned long long>((long long) (-dEdR2.x*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR2.y*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom2+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (-dEdR2.z*0x100000000)));\n"
"#endif\n"
"    }\n"
"#endif\n"
"#ifdef INCLUDE_ENERGY\n"
"    energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] += energy;\n"
"#endif\n"
"    SAVE_DERIVATIVES\n"
"}";
const string CudaKernelSources::nonbondedExceptions = "float4 exceptionParams = PARAMS[index];\n"
"real3 delta = make_real3(pos2.x-pos1.x, pos2.y-pos1.y, pos2.z-pos1.z);\n"
"#if APPLY_PERIODIC\n"
"APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"real invR = RSQRT(r2);\n"
"real sig2 = invR*exceptionParams.y;\n"
"sig2 *= sig2;\n"
"real sig6 = sig2*sig2*sig2;\n"
"real dEdR = exceptionParams.z*(12.0f*sig6-6.0f)*sig6;\n"
"real tempEnergy = exceptionParams.z*(sig6-1.0f)*sig6;\n"
"dEdR += exceptionParams.x*invR;\n"
"dEdR *= invR*invR;\n"
"tempEnergy += exceptionParams.x*invR;\n"
"energy += tempEnergy;\n"
"delta *= dEdR;\n"
"real3 force1 = -delta;\n"
"real3 force2 = delta;\n"
"";
const string CudaKernelSources::nonbondedParameters = "/**\n"
" * Compute the nonbonded parameters for particles and exceptions.\n"
" */\n"
"extern \"C\" __global__ void computeParameters(mixed* __restrict__ energyBuffer, bool includeSelfEnergy, real* __restrict__ globalParams,\n"
"        int numAtoms, const float4* __restrict__ baseParticleParams, real4* __restrict__ posq, real* __restrict__ charge,\n"
"        float2* __restrict__ sigmaEpsilon, float4* __restrict__ particleParamOffsets, int* __restrict__ particleOffsetIndices\n"
"#ifdef HAS_EXCEPTIONS\n"
"        , int numExceptions, const float4* __restrict__ baseExceptionParams, float4* __restrict__ exceptionParams,\n"
"        float4* __restrict__ exceptionParamOffsets, int* __restrict__ exceptionOffsetIndices\n"
"#endif\n"
"        ) {\n"
"    mixed energy = 0;\n"
"\n"
"    // Compute particle parameters.\n"
"    \n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numAtoms; i += blockDim.x*gridDim.x) {\n"
"        float4 params = baseParticleParams[i];\n"
"#ifdef HAS_OFFSETS\n"
"        int start = particleOffsetIndices[i], end = particleOffsetIndices[i+1];\n"
"        for (int j = start; j < end; j++) {\n"
"            float4 offset = particleParamOffsets[j];\n"
"            real value = globalParams[(int) offset.w];\n"
"            params.x += value*offset.x;\n"
"            params.y += value*offset.y;\n"
"            params.z += value*offset.z;\n"
"        }\n"
"#endif\n"
"#ifdef USE_POSQ_CHARGES\n"
"        posq[i].w = params.x;\n"
"#else\n"
"        charge[i] = params.x;\n"
"#endif\n"
"        sigmaEpsilon[i] = make_float2(0.5f*params.y, 2*SQRT(params.z));\n"
"#ifdef HAS_OFFSETS\n"
"    #ifdef INCLUDE_EWALD\n"
"        energy -= EWALD_SELF_ENERGY_SCALE*params.x*params.x;\n"
"    #endif\n"
"    #ifdef INCLUDE_LJPME\n"
"        real sig3 = params.y*params.y*params.y;\n"
"        energy += LJPME_SELF_ENERGY_SCALE*sig3*sig3*params.z;\n"
"    #endif\n"
"#endif\n"
"    }\n"
"\n"
"    // Compute exception parameters.\n"
"    \n"
"#ifdef HAS_EXCEPTIONS\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numExceptions; i += blockDim.x*gridDim.x) {\n"
"        float4 params = baseExceptionParams[i];\n"
"#ifdef HAS_OFFSETS\n"
"        int start = exceptionOffsetIndices[i], end = exceptionOffsetIndices[i+1];\n"
"        for (int j = start; j < end; j++) {\n"
"            float4 offset = exceptionParamOffsets[j];\n"
"            real value = globalParams[(int) offset.w];\n"
"            params.x += value*offset.x;\n"
"            params.y += value*offset.y;\n"
"            params.z += value*offset.z;\n"
"        }\n"
"#endif\n"
"        exceptionParams[i] = make_float4((float) (ONE_4PI_EPS0*params.x), (float) params.y, (float) (4*params.z), 0);\n"
"    }\n"
"#endif\n"
"    if (includeSelfEnergy)\n"
"        energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] += energy;\n"
"}\n"
"\n"
"/**\n"
" * Compute parameters for subtracting the reciprocal part of excluded interactions.\n"
" */\n"
"extern \"C\" __global__ void computeExclusionParameters(real4* __restrict__ posq, real* __restrict__ charge, float2* __restrict__ sigmaEpsilon,\n"
"        int numExclusions, const int2* __restrict__ exclusionAtoms, float4* __restrict__ exclusionParams) {\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < numExclusions; i += blockDim.x*gridDim.x) {\n"
"        int2 atoms = exclusionAtoms[i];\n"
"#ifdef USE_POSQ_CHARGES\n"
"        real chargeProd = posq[atoms.x].w*posq[atoms.y].w;\n"
"#else\n"
"        real chargeProd = charge[atoms.x]*charge[atoms.y];\n"
"#endif\n"
"#ifdef INCLUDE_LJPME\n"
"        float2 sigEps1 = sigmaEpsilon[atoms.x];\n"
"        float2 sigEps2 = sigmaEpsilon[atoms.y];\n"
"        float sigma = sigEps1.x*sigEps2.x;\n"
"        float epsilon = sigEps1.y*sigEps2.y;\n"
"#else\n"
"        float sigma = 0;\n"
"        float epsilon = 0;\n"
"#endif\n"
"        exclusionParams[i] = make_float4((float) (ONE_4PI_EPS0*chargeProd), sigma, epsilon, 0);\n"
"    }\n"
"}";
const string CudaKernelSources::parallel = "/**\n"
" * Sum the forces computed by different contexts.\n"
" */\n"
"\n"
"extern \"C\" __global__ void sumForces(long long* __restrict__ force, long long* __restrict__ buffer, int bufferSize, int numBuffers) {\n"
"    int totalSize = bufferSize*numBuffers;\n"
"    for (int index = blockDim.x*blockIdx.x+threadIdx.x; index < bufferSize; index += blockDim.x*gridDim.x) {\n"
"        long long sum = force[index];\n"
"        for (int i = index; i < totalSize; i += bufferSize)\n"
"            sum += buffer[i];\n"
"        force[index] = sum;\n"
"    }\n"
"}\n"
"";
const string CudaKernelSources::pme = "extern \"C\" __global__ void findAtomGridIndex(const real4* __restrict__ posq, int2* __restrict__ pmeAtomGridIndex,\n"
"            real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"            real3 recipBoxVecX, real3 recipBoxVecY, real3 recipBoxVecZ) {\n"
"    // Compute the index of the grid point each atom is associated with.\n"
"    \n"
"    for (int atom = blockIdx.x*blockDim.x+threadIdx.x; atom < NUM_ATOMS; atom += blockDim.x*gridDim.x) {\n"
"        real4 pos = posq[atom];\n"
"        APPLY_PERIODIC_TO_POS(pos)\n"
"        real3 t = make_real3(pos.x*recipBoxVecX.x+pos.y*recipBoxVecY.x+pos.z*recipBoxVecZ.x,\n"
"                             pos.y*recipBoxVecY.y+pos.z*recipBoxVecZ.y,\n"
"                             pos.z*recipBoxVecZ.z);\n"
"        t.x = (t.x-floor(t.x))*GRID_SIZE_X;\n"
"        t.y = (t.y-floor(t.y))*GRID_SIZE_Y;\n"
"        t.z = (t.z-floor(t.z))*GRID_SIZE_Z;\n"
"        int3 gridIndex = make_int3(((int) t.x) % GRID_SIZE_X,\n"
"                                   ((int) t.y) % GRID_SIZE_Y,\n"
"                                   ((int) t.z) % GRID_SIZE_Z);\n"
"        pmeAtomGridIndex[atom] = make_int2(atom, gridIndex.x*GRID_SIZE_Y*GRID_SIZE_Z+gridIndex.y*GRID_SIZE_Z+gridIndex.z);\n"
"    }\n"
"}\n"
"\n"
"extern \"C\" __global__ void gridSpreadCharge(const real4* __restrict__ posq, real* __restrict__ originalPmeGrid,\n"
"        real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        real3 recipBoxVecX, real3 recipBoxVecY, real3 recipBoxVecZ, const int2* __restrict__ pmeAtomGridIndex\n"
"#ifdef CHARGE_FROM_SIGEPS\n"
"        , const float2* __restrict__ sigmaEpsilon\n"
"#else\n"
"        , const real* __restrict__ charges\n"
"#endif\n"
"        ) {\n"
"    // To improve memory efficiency, we divide indices along the z axis into\n"
"    // PME_ORDER blocks, where the data for each block is stored together.  We\n"
"    // can ensure that all threads write to the same block at the same time,\n"
"    // which leads to better coalescing of writes.\n"
"    \n"
"    __shared__ int zindexTable[GRID_SIZE_Z+PME_ORDER];\n"
"    int blockSize = (int) ceil(GRID_SIZE_Z/(real) PME_ORDER);\n"
"    for (int i = threadIdx.x; i < GRID_SIZE_Z+PME_ORDER; i += blockDim.x) {\n"
"        int zindex = i % GRID_SIZE_Z;\n"
"	int block = zindex % PME_ORDER;\n"
"        zindexTable[i] = zindex/PME_ORDER + block*GRID_SIZE_X*GRID_SIZE_Y*blockSize;\n"
"    }\n"
"    __syncthreads();\n"
"    \n"
"    // Process the atoms in spatially sorted order.  This improves efficiency when writing\n"
"    // the grid values.\n"
"    \n"
"    real3 data[PME_ORDER];\n"
"    const real scale = RECIP(PME_ORDER-1);\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x) {\n"
"        int atom = pmeAtomGridIndex[i].x;\n"
"        real4 pos = posq[atom];\n"
"#ifdef CHARGE_FROM_SIGEPS\n"
"        const float2 sigEps = sigmaEpsilon[atom];\n"
"        const real charge = 8*sigEps.x*sigEps.x*sigEps.x*sigEps.y;\n"
"#else\n"
"        const real charge = (CHARGE)*EPSILON_FACTOR;\n"
"#endif\n"
"        APPLY_PERIODIC_TO_POS(pos)\n"
"        real3 t = make_real3(pos.x*recipBoxVecX.x+pos.y*recipBoxVecY.x+pos.z*recipBoxVecZ.x,\n"
"                             pos.y*recipBoxVecY.y+pos.z*recipBoxVecZ.y,\n"
"                             pos.z*recipBoxVecZ.z);\n"
"        t.x = (t.x-floor(t.x))*GRID_SIZE_X;\n"
"        t.y = (t.y-floor(t.y))*GRID_SIZE_Y;\n"
"        t.z = (t.z-floor(t.z))*GRID_SIZE_Z;\n"
"        int3 gridIndex = make_int3(((int) t.x) % GRID_SIZE_X,\n"
"                                   ((int) t.y) % GRID_SIZE_Y,\n"
"                                   ((int) t.z) % GRID_SIZE_Z);\n"
"        if (charge == 0)\n"
"            continue;\n"
"\n"
"        // Since we need the full set of thetas, it's faster to compute them here than load them\n"
"        // from global memory.\n"
"        \n"
"        real3 dr = make_real3(t.x-(int) t.x, t.y-(int) t.y, t.z-(int) t.z);\n"
"        data[PME_ORDER-1] = make_real3(0);\n"
"        data[1] = dr;\n"
"        data[0] = make_real3(1)-dr;\n"
"        for (int j = 3; j < PME_ORDER; j++) {\n"
"            real div = RECIP(j-1);\n"
"            data[j-1] = div*dr*data[j-2];\n"
"            for (int k = 1; k < (j-1); k++)\n"
"                data[j-k-1] = div*((dr+make_real3(k))*data[j-k-2] + (make_real3(j-k)-dr)*data[j-k-1]);\n"
"            data[0] = div*(make_real3(1)-dr)*data[0];\n"
"        }\n"
"        data[PME_ORDER-1] = scale*dr*data[PME_ORDER-2];\n"
"        for (int j = 1; j < (PME_ORDER-1); j++)\n"
"            data[PME_ORDER-j-1] = scale*((dr+make_real3(j))*data[PME_ORDER-j-2] + (make_real3(PME_ORDER-j)-dr)*data[PME_ORDER-j-1]);\n"
"        data[0] = scale*(make_real3(1)-dr)*data[0];\n"
"        \n"
"        // Spread the charge from this atom onto each grid point.\n"
"\n"
"	int izoffset = (PME_ORDER-(gridIndex.z%PME_ORDER)) % PME_ORDER;\n"
"        for (int ix = 0; ix < PME_ORDER; ix++) {\n"
"            int xbase = gridIndex.x+ix;\n"
"            xbase -= (xbase >= GRID_SIZE_X ? GRID_SIZE_X : 0);\n"
"            xbase = xbase*GRID_SIZE_Y;\n"
"            real dx = charge*data[ix].x;\n"
"            for (int iy = 0; iy < PME_ORDER; iy++) {\n"
"                int ybase = gridIndex.y+iy;\n"
"                ybase -= (ybase >= GRID_SIZE_Y ? GRID_SIZE_Y : 0);\n"
"                ybase = (xbase+ybase)*blockSize;\n"
"                real dxdy = dx*data[iy].y;\n"
"                for (int i = 0; i < PME_ORDER; i++) {\n"
"		    int iz = (i+izoffset) % PME_ORDER;\n"
"                    int zindex = gridIndex.z+iz;\n"
"                    int index = ybase + zindexTable[zindex];\n"
"                    real add = dxdy*data[iz].z;\n"
"#if defined(USE_DOUBLE_PRECISION) || defined(USE_DETERMINISTIC_FORCES)\n"
"                    unsigned long long * ulonglong_p = (unsigned long long *) originalPmeGrid;\n"
"                    atomicAdd(&ulonglong_p[index],  static_cast<unsigned long long>((long long) (add*0x100000000)));\n"
"#else\n"
"                    atomicAdd(&originalPmeGrid[index], add);\n"
"#endif\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"extern \"C\" __global__ void finishSpreadCharge(\n"
"#if defined(USE_DOUBLE_PRECISION) || defined(USE_DETERMINISTIC_FORCES)\n"
"        const long long* __restrict__ grid1,\n"
"#else\n"
"        const real* __restrict__ grid1,\n"
"#endif\n"
"        real* __restrict__ grid2) {\n"
"    // During charge spreading, we shuffled the order of indices along the z\n"
"    // axis to make memory access more efficient.  We now need to unshuffle\n"
"    // them.  If the values were accumulated as fixed point, we also need to\n"
"    // convert them to floating point.\n"
"\n"
"    __shared__ int zindexTable[GRID_SIZE_Z];\n"
"    int blockSize = (int) ceil(GRID_SIZE_Z/(real) PME_ORDER);\n"
"    for (int i = threadIdx.x; i < GRID_SIZE_Z; i += blockDim.x) {\n"
"	int block = i % PME_ORDER;\n"
"        zindexTable[i] = i/PME_ORDER + block*GRID_SIZE_X*GRID_SIZE_Y*blockSize;\n"
"    }\n"
"    __syncthreads();\n"
"    const unsigned int gridSize = GRID_SIZE_X*GRID_SIZE_Y*GRID_SIZE_Z;\n"
"    real scale = 1/(real) 0x100000000;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        int zindex = index%GRID_SIZE_Z;\n"
"        int loadIndex = zindexTable[zindex] + blockSize*(int) (index/GRID_SIZE_Z);\n"
"#if defined(USE_DOUBLE_PRECISION) || defined(USE_DETERMINISTIC_FORCES)\n"
"        grid2[index] = scale*grid1[loadIndex];\n"
"#else\n"
"        grid2[index] = grid1[loadIndex];\n"
"#endif\n"
"    }\n"
"}\n"
"\n"
"// convolutes on the halfcomplex_pmeGrid, which is of size NX*NY*(NZ/2+1) as F(Q) is conjugate symmetric\n"
"extern \"C\" __global__ void \n"
"reciprocalConvolution(real2* __restrict__ halfcomplex_pmeGrid, mixed* __restrict__ energyBuffer, \n"
"                      const real* __restrict__ pmeBsplineModuliX, const real* __restrict__ pmeBsplineModuliY, const real* __restrict__ pmeBsplineModuliZ, \n"
"                      real4 periodicBoxSize, real3 recipBoxVecX, real3 recipBoxVecY, real3 recipBoxVecZ) {\n"
"    // R2C stores into a half complex matrix where the last dimension is cut by half\n"
"    const unsigned int gridSize = GRID_SIZE_X*GRID_SIZE_Y*(GRID_SIZE_Z/2+1);\n"
"#ifdef USE_LJPME\n"
"    const real recipScaleFactor = -2*M_PI*SQRT(M_PI)*RECIP(6*periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"    real bfac = M_PI / EWALD_ALPHA;\n"
"    real fac1 = 2*M_PI*M_PI*M_PI*SQRT(M_PI);\n"
"    real fac2 = EWALD_ALPHA*EWALD_ALPHA*EWALD_ALPHA;\n"
"    real fac3 = -2*EWALD_ALPHA*M_PI*M_PI;\n"
"#else\n"
"    const real recipScaleFactor = RECIP(M_PI*periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"#endif\n"
"\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        // real indices\n"
"        int kx = index/(GRID_SIZE_Y*(GRID_SIZE_Z/2+1));\n"
"        int remainder = index-kx*GRID_SIZE_Y*(GRID_SIZE_Z/2+1);\n"
"        int ky = remainder/(GRID_SIZE_Z/2+1);\n"
"        int kz = remainder-ky*(GRID_SIZE_Z/2+1);\n"
"        int mx = (kx < (GRID_SIZE_X+1)/2) ? kx : (kx-GRID_SIZE_X);\n"
"        int my = (ky < (GRID_SIZE_Y+1)/2) ? ky : (ky-GRID_SIZE_Y);\n"
"        int mz = (kz < (GRID_SIZE_Z+1)/2) ? kz : (kz-GRID_SIZE_Z);\n"
"        real mhx = mx*recipBoxVecX.x;\n"
"        real mhy = mx*recipBoxVecY.x+my*recipBoxVecY.y;\n"
"        real mhz = mx*recipBoxVecZ.x+my*recipBoxVecZ.y+mz*recipBoxVecZ.z;\n"
"        real bx = pmeBsplineModuliX[kx];\n"
"        real by = pmeBsplineModuliY[ky];\n"
"        real bz = pmeBsplineModuliZ[kz];\n"
"        real2 grid = halfcomplex_pmeGrid[index];\n"
"        real m2 = mhx*mhx+mhy*mhy+mhz*mhz;\n"
"#ifdef USE_LJPME\n"
"        real denom = recipScaleFactor/(bx*by*bz);\n"
"        real m = SQRT(m2);\n"
"        real m3 = m*m2;\n"
"        real b = bfac*m;\n"
"        real expfac = -b*b;\n"
"        real expterm = EXP(expfac);\n"
"        real erfcterm = ERFC(b);\n"
"        real eterm = (fac1*erfcterm*m3 + expterm*(fac2 + fac3*m2)) * denom;\n"
"        halfcomplex_pmeGrid[index] = make_real2(grid.x*eterm, grid.y*eterm);\n"
"#else\n"
"        real denom = m2*bx*by*bz;\n"
"        real eterm = recipScaleFactor*EXP(-RECIP_EXP_FACTOR*m2)/denom;\n"
"        if (kx != 0 || ky != 0 || kz != 0) {\n"
"            halfcomplex_pmeGrid[index] = make_real2(grid.x*eterm, grid.y*eterm);\n"
"        }\n"
"#endif\n"
"    }\n"
"}\n"
"\n"
"\n"
"extern \"C\" __global__ void\n"
"gridEvaluateEnergy(real2* __restrict__ halfcomplex_pmeGrid, mixed* __restrict__ energyBuffer,\n"
"                      const real* __restrict__ pmeBsplineModuliX, const real* __restrict__ pmeBsplineModuliY, const real* __restrict__ pmeBsplineModuliZ,\n"
"                      real4 periodicBoxSize, real3 recipBoxVecX, real3 recipBoxVecY, real3 recipBoxVecZ) {\n"
"    // R2C stores into a half complex matrix where the last dimension is cut by half\n"
"    const unsigned int gridSize = GRID_SIZE_X*GRID_SIZE_Y*GRID_SIZE_Z;\n"
" #ifdef USE_LJPME\n"
"    const real recipScaleFactor = -2*M_PI*SQRT(M_PI)*RECIP(6*periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"    real bfac = M_PI / EWALD_ALPHA;\n"
"    real fac1 = 2*M_PI*M_PI*M_PI*SQRT(M_PI);\n"
"    real fac2 = EWALD_ALPHA*EWALD_ALPHA*EWALD_ALPHA;\n"
"    real fac3 = -2*EWALD_ALPHA*M_PI*M_PI;\n"
"#else\n"
"    const real recipScaleFactor = RECIP(M_PI*periodicBoxSize.x*periodicBoxSize.y*periodicBoxSize.z);\n"
"#endif\n"
"\n"
"    mixed energy = 0;\n"
"    for (int index = blockIdx.x*blockDim.x+threadIdx.x; index < gridSize; index += blockDim.x*gridDim.x) {\n"
"        // real indices\n"
"        int kx = index/(GRID_SIZE_Y*(GRID_SIZE_Z));\n"
"        int remainder = index-kx*GRID_SIZE_Y*(GRID_SIZE_Z);\n"
"        int ky = remainder/(GRID_SIZE_Z);\n"
"        int kz = remainder-ky*(GRID_SIZE_Z);\n"
"        int mx = (kx < (GRID_SIZE_X+1)/2) ? kx : (kx-GRID_SIZE_X);\n"
"        int my = (ky < (GRID_SIZE_Y+1)/2) ? ky : (ky-GRID_SIZE_Y);\n"
"        int mz = (kz < (GRID_SIZE_Z+1)/2) ? kz : (kz-GRID_SIZE_Z);\n"
"        real mhx = mx*recipBoxVecX.x;\n"
"        real mhy = mx*recipBoxVecY.x+my*recipBoxVecY.y;\n"
"        real mhz = mx*recipBoxVecZ.x+my*recipBoxVecZ.y+mz*recipBoxVecZ.z;\n"
"        real m2 = mhx*mhx+mhy*mhy+mhz*mhz;\n"
"        real bx = pmeBsplineModuliX[kx];\n"
"        real by = pmeBsplineModuliY[ky];\n"
"        real bz = pmeBsplineModuliZ[kz];\n"
"#ifdef USE_LJPME\n"
"        real denom = recipScaleFactor/(bx*by*bz);\n"
"        real m = SQRT(m2);\n"
"        real m3 = m*m2;\n"
"        real b = bfac*m;\n"
"        real expfac = -b*b;\n"
"        real expterm = EXP(expfac);\n"
"        real erfcterm = ERFC(b);\n"
"        real eterm = (fac1*erfcterm*m3 + expterm*(fac2 + fac3*m2)) * denom;\n"
"#else\n"
"        real denom = m2*bx*by*bz;\n"
"        real eterm = recipScaleFactor*EXP(-RECIP_EXP_FACTOR*m2)/denom;\n"
"#endif\n"
"\n"
"        if (kz >= (GRID_SIZE_Z/2+1)) {\n"
"            kx = ((kx == 0) ? kx : GRID_SIZE_X-kx);\n"
"            ky = ((ky == 0) ? ky : GRID_SIZE_Y-ky);\n"
"            kz = GRID_SIZE_Z-kz;\n"
"        } \n"
"        int indexInHalfComplexGrid = kz + ky*(GRID_SIZE_Z/2+1)+kx*(GRID_SIZE_Y*(GRID_SIZE_Z/2+1));\n"
"        real2 grid = halfcomplex_pmeGrid[indexInHalfComplexGrid];\n"
"#ifndef USE_LJPME\n"
"        if (kx != 0 || ky != 0 || kz != 0)\n"
"#endif\n"
"            energy += eterm*(grid.x*grid.x + grid.y*grid.y);\n"
"    }\n"
"#if defined(USE_PME_STREAM) && !defined(USE_LJPME)\n"
"    energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] = 0.5f*energy;\n"
"#else\n"
"    energyBuffer[blockIdx.x*blockDim.x+threadIdx.x] += 0.5f*energy;\n"
"#endif\n"
"}\n"
"\n"
"extern \"C\" __global__\n"
"void gridInterpolateForce(const real4* __restrict__ posq, unsigned long long* __restrict__ forceBuffers, const real* __restrict__ originalPmeGrid,\n"
"        real4 periodicBoxSize, real4 invPeriodicBoxSize, real4 periodicBoxVecX, real4 periodicBoxVecY, real4 periodicBoxVecZ,\n"
"        real3 recipBoxVecX, real3 recipBoxVecY, real3 recipBoxVecZ, const int2* __restrict__ pmeAtomGridIndex\n"
"#ifdef CHARGE_FROM_SIGEPS\n"
"        , const float2* __restrict__ sigmaEpsilon\n"
"#else\n"
"        , const real* __restrict__ charges\n"
"#endif\n"
"        ) {\n"
"    real3 data[PME_ORDER];\n"
"    real3 ddata[PME_ORDER];\n"
"    const real scale = RECIP(PME_ORDER-1);\n"
"    \n"
"    // Process the atoms in spatially sorted order.  This improves cache performance when loading\n"
"    // the grid values.\n"
"    \n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < NUM_ATOMS; i += blockDim.x*gridDim.x) {\n"
"        int atom = pmeAtomGridIndex[i].x;\n"
"        real3 force = make_real3(0);\n"
"        real4 pos = posq[atom];\n"
"        APPLY_PERIODIC_TO_POS(pos)\n"
"        real3 t = make_real3(pos.x*recipBoxVecX.x+pos.y*recipBoxVecY.x+pos.z*recipBoxVecZ.x,\n"
"                             pos.y*recipBoxVecY.y+pos.z*recipBoxVecZ.y,\n"
"                             pos.z*recipBoxVecZ.z);\n"
"        t.x = (t.x-floor(t.x))*GRID_SIZE_X;\n"
"        t.y = (t.y-floor(t.y))*GRID_SIZE_Y;\n"
"        t.z = (t.z-floor(t.z))*GRID_SIZE_Z;\n"
"        int3 gridIndex = make_int3(((int) t.x) % GRID_SIZE_X,\n"
"                                   ((int) t.y) % GRID_SIZE_Y,\n"
"                                   ((int) t.z) % GRID_SIZE_Z);\n"
"\n"
"        // Since we need the full set of thetas, it's faster to compute them here than load them\n"
"        // from global memory.\n"
"        \n"
"        real3 dr = make_real3(t.x-(int) t.x, t.y-(int) t.y, t.z-(int) t.z);\n"
"        data[PME_ORDER-1] = make_real3(0);\n"
"        data[1] = dr;\n"
"        data[0] = make_real3(1)-dr;\n"
"        for (int j = 3; j < PME_ORDER; j++) {\n"
"            real div = RECIP(j-1);\n"
"            data[j-1] = div*dr*data[j-2];\n"
"            for (int k = 1; k < (j-1); k++)\n"
"                data[j-k-1] = div*((dr+make_real3(k))*data[j-k-2] + (make_real3(j-k)-dr)*data[j-k-1]);\n"
"            data[0] = div*(make_real3(1)-dr)*data[0];\n"
"        }\n"
"        ddata[0] = -data[0];\n"
"        for (int j = 1; j < PME_ORDER; j++)\n"
"            ddata[j] = data[j-1]-data[j];\n"
"        data[PME_ORDER-1] = scale*dr*data[PME_ORDER-2];\n"
"        for (int j = 1; j < (PME_ORDER-1); j++)\n"
"            data[PME_ORDER-j-1] = scale*((dr+make_real3(j))*data[PME_ORDER-j-2] + (make_real3(PME_ORDER-j)-dr)*data[PME_ORDER-j-1]);\n"
"        data[0] = scale*(make_real3(1)-dr)*data[0];\n"
"        \n"
"        // Compute the force on this atom.\n"
"         \n"
"        for (int ix = 0; ix < PME_ORDER; ix++) {\n"
"            int xbase = gridIndex.x+ix;\n"
"            xbase -= (xbase >= GRID_SIZE_X ? GRID_SIZE_X : 0);\n"
"            xbase = xbase*GRID_SIZE_Y*GRID_SIZE_Z;\n"
"            real dx = data[ix].x;\n"
"            real ddx = ddata[ix].x;\n"
"            \n"
"            for (int iy = 0; iy < PME_ORDER; iy++) {\n"
"                int ybase = gridIndex.y+iy;\n"
"                ybase -= (ybase >= GRID_SIZE_Y ? GRID_SIZE_Y : 0);\n"
"                ybase = xbase + ybase*GRID_SIZE_Z;\n"
"                real dy = data[iy].y;\n"
"                real ddy = ddata[iy].y;\n"
"                \n"
"                for (int iz = 0; iz < PME_ORDER; iz++) {\n"
"                    int zindex = gridIndex.z+iz;\n"
"                    zindex -= (zindex >= GRID_SIZE_Z ? GRID_SIZE_Z : 0);\n"
"                    int index = ybase + zindex;\n"
"                    real gridvalue = originalPmeGrid[index];\n"
"                    force.x += ddx*dy*data[iz].z*gridvalue;\n"
"                    force.y += dx*ddy*data[iz].z*gridvalue;\n"
"                    force.z += dx*dy*ddata[iz].z*gridvalue;\n"
"                }\n"
"            }\n"
"        }\n"
"#ifdef CHARGE_FROM_SIGEPS\n"
"        const float2 sigEps = sigmaEpsilon[atom];\n"
"        real q = 8*sigEps.x*sigEps.x*sigEps.x*sigEps.y;\n"
"#else\n"
"        real q = CHARGE*EPSILON_FACTOR;\n"
"#endif\n"
"        real forceX = -q*(force.x*GRID_SIZE_X*recipBoxVecX.x);\n"
"        real forceY = -q*(force.x*GRID_SIZE_X*recipBoxVecY.x+force.y*GRID_SIZE_Y*recipBoxVecY.y);\n"
"        real forceZ = -q*(force.x*GRID_SIZE_X*recipBoxVecZ.x+force.y*GRID_SIZE_Y*recipBoxVecZ.y+force.z*GRID_SIZE_Z*recipBoxVecZ.z);\n"
"        atomicAdd(&forceBuffers[atom], static_cast<unsigned long long>((long long) (forceX*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom+PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (forceY*0x100000000)));\n"
"        atomicAdd(&forceBuffers[atom+2*PADDED_NUM_ATOMS], static_cast<unsigned long long>((long long) (forceZ*0x100000000)));\n"
"    }\n"
"}\n"
"\n"
"extern \"C\" __global__\n"
"void addForces(const real4* __restrict__ forces, unsigned long long* __restrict__ forceBuffers) {\n"
"    for (int atom = blockIdx.x*blockDim.x+threadIdx.x; atom < NUM_ATOMS; atom += blockDim.x*gridDim.x) {\n"
"        real4 f = forces[atom];\n"
"        forceBuffers[atom] += static_cast<unsigned long long>((long long) (f.x*0x100000000));\n"
"        forceBuffers[atom+PADDED_NUM_ATOMS] += static_cast<unsigned long long>((long long) (f.y*0x100000000));\n"
"        forceBuffers[atom+2*PADDED_NUM_ATOMS] += static_cast<unsigned long long>((long long) (f.z*0x100000000));\n"
"    }\n"
"}\n"
"\n"
"extern \"C\" __global__\n"
"void addEnergy(const mixed* __restrict__ pmeEnergyBuffer, mixed* __restrict__ energyBuffer, int bufferSize) {\n"
"    for (int i = blockIdx.x*blockDim.x+threadIdx.x; i < bufferSize; i += blockDim.x*gridDim.x)\n"
"        energyBuffer[i] += pmeEnergyBuffer[i];\n"
"}\n"
"";
const string CudaKernelSources::pmeExclusions = "const float4 exclusionParams = PARAMS[index];\n"
"real3 delta = make_real3(pos2.x-pos1.x, pos2.y-pos1.y, pos2.z-pos1.z);\n"
"#if USE_PERIODIC\n"
"    APPLY_PERIODIC_TO_DELTA(delta)\n"
"#endif\n"
"const real r2 = delta.x*delta.x + delta.y*delta.y + delta.z*delta.z;\n"
"const real r = SQRT(r2);\n"
"const real invR = RECIP(r);\n"
"const real alphaR = EWALD_ALPHA*r;\n"
"const real expAlphaRSqr = EXP(-alphaR*alphaR);\n"
"real tempForce = 0.0f;\n"
"if (alphaR > 1e-6f) {\n"
"    const real erfAlphaR = ERF(alphaR);\n"
"    const real prefactor = exclusionParams.x*invR;\n"
"    tempForce = -prefactor*(erfAlphaR-alphaR*expAlphaRSqr*TWO_OVER_SQRT_PI);\n"
"    energy -= prefactor*erfAlphaR;\n"
"}\n"
"else {\n"
"    energy -= TWO_OVER_SQRT_PI*EWALD_ALPHA*exclusionParams.x;\n"
"}\n"
"#if DO_LJPME\n"
"const real dispersionAlphaR = EWALD_DISPERSION_ALPHA*r;\n"
"const real dar2 = dispersionAlphaR*dispersionAlphaR;\n"
"const real dar4 = dar2*dar2;\n"
"const real dar6 = dar4*dar2;\n"
"const real invR2 = invR*invR;\n"
"const real expDar2 = EXP(-dar2);\n"
"const real c6 = 64*exclusionParams.y*exclusionParams.y*exclusionParams.y*exclusionParams.z;\n"
"const real coef = invR2*invR2*invR2*c6;\n"
"const real eprefac = 1.0f + dar2 + 0.5f*dar4;\n"
"const real dprefac = eprefac + dar6/6.0f;\n"
"energy += coef*(1.0f - expDar2*eprefac);\n"
"tempForce += 6.0f*coef*(1.0f - expDar2*dprefac);\n"
"#endif\n"
"if (r > 0)\n"
"    delta *= tempForce*invR*invR;\n"
"real3 force1 = -delta;\n"
"real3 force2 = delta;\n"
"";
const string CudaKernelSources::sort = "__device__ KEY_TYPE getValue(DATA_TYPE value) {\n"
"    return SORT_KEY;\n"
"}\n"
"\n"
"extern \"C\" {\n"
"\n"
"/**\n"
" * Sort a list that is short enough to entirely fit in local memory.  This is executed as\n"
" * a single thread block.\n"
" */\n"
"__global__ void sortShortList(DATA_TYPE* __restrict__ data, unsigned int length) {\n"
"    // Load the data into local memory.\n"
"    \n"
"    extern __shared__ DATA_TYPE dataBuffer[];\n"
"    for (int index = threadIdx.x; index < length; index += blockDim.x)\n"
"        dataBuffer[index] = data[index];\n"
"    __syncthreads();\n"
"\n"
"    // Perform a bitonic sort in local memory.\n"
"\n"
"    for (unsigned int k = 2; k < 2*length; k *= 2) {\n"
"        for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"            for (unsigned int i = threadIdx.x; i < length; i += blockDim.x) {\n"
"                int ixj = i^j;\n"
"                if (ixj > i && ixj < length) {\n"
"                    DATA_TYPE value1 = dataBuffer[i];\n"
"                    DATA_TYPE value2 = dataBuffer[ixj];\n"
"                    bool ascending = ((i&k) == 0);\n"
"                    for (unsigned int mask = k*2; mask < 2*length; mask *= 2)\n"
"                        ascending = ((i&mask) == 0 ? !ascending : ascending);\n"
"                    KEY_TYPE lowKey  = (ascending ? getValue(value1) : getValue(value2));\n"
"                    KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                    if (lowKey > highKey) {\n"
"                        dataBuffer[i] = value2;\n"
"                        dataBuffer[ixj] = value1;\n"
"                    }\n"
"                }\n"
"            }\n"
"            __syncthreads();\n"
"        }\n"
"    }\n"
"\n"
"    // Write the data back to global memory.\n"
"\n"
"    for (int index = threadIdx.x; index < length; index += blockDim.x)\n"
"        data[index] = dataBuffer[index];\n"
"}\n"
"\n"
"/**\n"
" * An alternate kernel for sorting short lists.  In this version every thread does a full\n"
" * scan through the data to select the destination for one element.  This involves more\n"
" * work, but also parallelizes much better.\n"
" */\n"
"__global__ void sortShortList2(const DATA_TYPE* __restrict__ dataIn, DATA_TYPE* __restrict__ dataOut, unsigned int length) {\n"
"    __shared__ DATA_TYPE dataBuffer[64];\n"
"    int globalId = blockDim.x*blockIdx.x+threadIdx.x;\n"
"    DATA_TYPE value = dataIn[globalId < length ? globalId : 0];\n"
"    KEY_TYPE key = getValue(value);\n"
"    int count = 0;\n"
"    for (int blockStart = 0; blockStart < length; blockStart += blockDim.x) {\n"
"        int numInBlock = min(blockDim.x, length-blockStart);\n"
"        __syncthreads();\n"
"        if (threadIdx.x < numInBlock)\n"
"            dataBuffer[threadIdx.x] = dataIn[blockStart+threadIdx.x];\n"
"        __syncthreads();\n"
"        for (int i = 0; i < numInBlock; i++) {\n"
"            KEY_TYPE otherKey = getValue(dataBuffer[i]);\n"
"            if (otherKey < key || (otherKey == key && blockStart+i < globalId))\n"
"                count++;\n"
"        }\n"
"    }\n"
"    if (globalId < length)\n"
"        dataOut[count] = value;\n"
"}\n"
"\n"
"/**\n"
" * Calculate the minimum and maximum value in the array to be sorted.  This kernel\n"
" * is executed as a single work group.\n"
" */\n"
"__global__ void computeRange(const DATA_TYPE* __restrict__ data, unsigned int length, KEY_TYPE* __restrict__ range,\n"
"        unsigned int numBuckets, unsigned int* __restrict__ bucketOffset) {\n"
"    extern __shared__ KEY_TYPE minBuffer[];\n"
"    KEY_TYPE* maxBuffer = minBuffer+blockDim.x;\n"
"    KEY_TYPE minimum = MAX_KEY;\n"
"    KEY_TYPE maximum = MIN_KEY;\n"
"\n"
"    // Each thread calculates the range of a subset of values.\n"
"\n"
"    for (unsigned int index = threadIdx.x; index < length; index += blockDim.x) {\n"
"        KEY_TYPE value = getValue(data[index]);\n"
"        minimum = min(minimum, value);\n"
"        maximum = max(maximum, value);\n"
"    }\n"
"\n"
"    // Now reduce them.\n"
"\n"
"    minBuffer[threadIdx.x] = minimum;\n"
"    maxBuffer[threadIdx.x] = maximum;\n"
"    __syncthreads();\n"
"    for (unsigned int step = 1; step < blockDim.x; step *= 2) {\n"
"        if (threadIdx.x+step < blockDim.x && threadIdx.x%(2*step) == 0) {\n"
"            minBuffer[threadIdx.x] = min(minBuffer[threadIdx.x], minBuffer[threadIdx.x+step]);\n"
"            maxBuffer[threadIdx.x] = max(maxBuffer[threadIdx.x], maxBuffer[threadIdx.x+step]);\n"
"        }\n"
"        __syncthreads();\n"
"    }\n"
"    minimum = minBuffer[0];\n"
"    maximum = maxBuffer[0];\n"
"    if (threadIdx.x == 0) {\n"
"        range[0] = minimum;\n"
"        range[1] = maximum;\n"
"    }\n"
"    \n"
"    // Clear the bucket counters in preparation for the next kernel.\n"
"\n"
"    for (unsigned int index = threadIdx.x; index < numBuckets; index += blockDim.x)\n"
"        bucketOffset[index] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Assign elements to buckets.\n"
" */\n"
"__global__ void assignElementsToBuckets(const DATA_TYPE* __restrict__ data, unsigned int length, unsigned int numBuckets, const KEY_TYPE* __restrict__ range,\n"
"        unsigned int* __restrict__ bucketOffset, unsigned int* __restrict__ bucketOfElement, unsigned int* __restrict__ offsetInBucket) {\n"
"    float minValue = (float) (range[0]);\n"
"    float maxValue = (float) (range[1]);\n"
"    float bucketWidth = (maxValue-minValue)/numBuckets;\n"
"    for (unsigned int index = blockDim.x*blockIdx.x+threadIdx.x; index < length; index += blockDim.x*gridDim.x) {\n"
"        float key = (float) getValue(data[index]);\n"
"        unsigned int bucketIndex = min((unsigned int) ((key-minValue)/bucketWidth), numBuckets-1);\n"
"        offsetInBucket[index] = atomicAdd(&bucketOffset[bucketIndex], 1);\n"
"        bucketOfElement[index] = bucketIndex;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Sum the bucket sizes to compute the start position of each bucket.  This kernel\n"
" * is executed as a single work group.\n"
" */\n"
"__global__ void computeBucketPositions(unsigned int numBuckets, unsigned int* __restrict__ bucketOffset) {\n"
"    extern __shared__ unsigned int posBuffer[];\n"
"    unsigned int globalOffset = 0;\n"
"    for (unsigned int startBucket = 0; startBucket < numBuckets; startBucket += blockDim.x) {\n"
"        // Load the bucket sizes into local memory.\n"
"\n"
"        unsigned int globalIndex = startBucket+threadIdx.x;\n"
"        __syncthreads();\n"
"        posBuffer[threadIdx.x] = (globalIndex < numBuckets ? bucketOffset[globalIndex] : 0);\n"
"        __syncthreads();\n"
"\n"
"        // Perform a parallel prefix sum.\n"
"\n"
"        for (unsigned int step = 1; step < blockDim.x; step *= 2) {\n"
"            unsigned int add = (threadIdx.x >= step ? posBuffer[threadIdx.x-step] : 0);\n"
"            __syncthreads();\n"
"            posBuffer[threadIdx.x] += add;\n"
"            __syncthreads();\n"
"        }\n"
"\n"
"        // Write the results back to global memory.\n"
"\n"
"        if (globalIndex < numBuckets)\n"
"            bucketOffset[globalIndex] = posBuffer[threadIdx.x]+globalOffset;\n"
"        globalOffset += posBuffer[blockDim.x-1];\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Copy the input data into the buckets for sorting.\n"
" */\n"
"__global__ void copyDataToBuckets(const DATA_TYPE* __restrict__ data, DATA_TYPE* __restrict__ buckets, unsigned int length, const unsigned int* __restrict__ bucketOffset, const unsigned int* __restrict__ bucketOfElement, const unsigned int* __restrict__ offsetInBucket) {\n"
"    for (unsigned int index = blockDim.x*blockIdx.x+threadIdx.x; index < length; index += blockDim.x*gridDim.x) {\n"
"        DATA_TYPE element = data[index];\n"
"        unsigned int bucketIndex = bucketOfElement[index];\n"
"        unsigned int offset = (bucketIndex == 0 ? 0 : bucketOffset[bucketIndex-1]);\n"
"        buckets[offset+offsetInBucket[index]] = element;\n"
"    }\n"
"}\n"
"\n"
"/**\n"
" * Sort the data in each bucket.\n"
" */\n"
"__global__ void sortBuckets(DATA_TYPE* __restrict__ data, const DATA_TYPE* __restrict__ buckets, unsigned int numBuckets, const unsigned int* __restrict__ bucketOffset) {\n"
"    extern __shared__ DATA_TYPE dataBuffer[];\n"
"    for (unsigned int index = blockIdx.x; index < numBuckets; index += gridDim.x) {\n"
"        unsigned int startIndex = (index == 0 ? 0 : bucketOffset[index-1]);\n"
"        unsigned int endIndex = bucketOffset[index];\n"
"        unsigned int length = endIndex-startIndex;\n"
"        if (length <= blockDim.x) {\n"
"            // Load the data into local memory.\n"
"\n"
"            if (threadIdx.x < length)\n"
"                dataBuffer[threadIdx.x] = buckets[startIndex+threadIdx.x];\n"
"            else\n"
"                dataBuffer[threadIdx.x] = MAX_VALUE;\n"
"            __syncthreads();\n"
"\n"
"            // Perform a bitonic sort in local memory.\n"
"\n"
"            for (unsigned int k = 2; k <= blockDim.x; k *= 2) {\n"
"                for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"                    int ixj = threadIdx.x^j;\n"
"                    if (ixj > threadIdx.x) {\n"
"                        DATA_TYPE value1 = dataBuffer[threadIdx.x];\n"
"                        DATA_TYPE value2 = dataBuffer[ixj];\n"
"                        bool ascending = (threadIdx.x&k) == 0;\n"
"                        KEY_TYPE lowKey = (ascending ? getValue(value1) : getValue(value2));\n"
"                        KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                        if (lowKey > highKey) {\n"
"                            dataBuffer[threadIdx.x] = value2;\n"
"                            dataBuffer[ixj] = value1;\n"
"                        }\n"
"                    }\n"
"                    __syncthreads();\n"
"                }\n"
"            }\n"
"\n"
"            // Write the data to the sorted array.\n"
"\n"
"            if (threadIdx.x < length)\n"
"                data[startIndex+threadIdx.x] = dataBuffer[threadIdx.x];\n"
"        }\n"
"        else {\n"
"            // Copy the bucket data over to the output array.\n"
"\n"
"            for (unsigned int i = threadIdx.x; i < length; i += blockDim.x)\n"
"                data[startIndex+i] = buckets[startIndex+i];\n"
"            __threadfence_block();\n"
"            __syncthreads();\n"
"\n"
"            // Perform a bitonic sort in global memory.\n"
"\n"
"            for (unsigned int k = 2; k < 2*length; k *= 2) {\n"
"                for (unsigned int j = k/2; j > 0; j /= 2) {\n"
"                    for (unsigned int i = threadIdx.x; i < length; i += blockDim.x) {\n"
"                        int ixj = i^j;\n"
"                        if (ixj > i && ixj < length) {\n"
"                            DATA_TYPE value1 = data[startIndex+i];\n"
"                            DATA_TYPE value2 = data[startIndex+ixj];\n"
"                            bool ascending = ((i&k) == 0);\n"
"                            for (unsigned int mask = k*2; mask < 2*length; mask *= 2)\n"
"                                ascending = ((i&mask) == 0 ? !ascending : ascending);\n"
"                            KEY_TYPE lowKey  = (ascending ? getValue(value1) : getValue(value2));\n"
"                            KEY_TYPE highKey = (ascending ? getValue(value2) : getValue(value1));\n"
"                            if (lowKey > highKey) {\n"
"                                data[startIndex+i] = value2;\n"
"                                data[startIndex+ixj] = value1;\n"
"                            }\n"
"                        }\n"
"                    }\n"
"                    __threadfence_block();\n"
"                    __syncthreads();\n"
"                }\n"
"            }\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"}";
const string CudaKernelSources::torsionForce = "const real PI = (real) 3.14159265358979323846;\n"
"real3 v0 = make_real3(pos1.x-pos2.x, pos1.y-pos2.y, pos1.z-pos2.z);\n"
"real3 v1 = make_real3(pos3.x-pos2.x, pos3.y-pos2.y, pos3.z-pos2.z);\n"
"real3 v2 = make_real3(pos3.x-pos4.x, pos3.y-pos4.y, pos3.z-pos4.z);\n"
"#if APPLY_PERIODIC\n"
"APPLY_PERIODIC_TO_DELTA(v0)\n"
"APPLY_PERIODIC_TO_DELTA(v1)\n"
"APPLY_PERIODIC_TO_DELTA(v2)\n"
"#endif\n"
"real3 cp0 = cross(v0, v1);\n"
"real3 cp1 = cross(v1, v2);\n"
"real cosangle = dot(normalize(cp0), normalize(cp1));\n"
"real theta;\n"
"if (cosangle > 0.99f || cosangle < -0.99f) {\n"
"    // We're close to the singularity in acos(), so take the cross product and use asin() instead.\n"
"\n"
"    real3 cross_prod = cross(cp0, cp1);\n"
"    real scale = dot(cp0, cp0)*dot(cp1, cp1);\n"
"    theta = ASIN(SQRT(dot(cross_prod, cross_prod)/scale));\n"
"    if (cosangle < 0)\n"
"        theta = PI-theta;\n"
"}\n"
"else\n"
"   theta = ACOS(cosangle);\n"
"theta = (dot(v0, cp1) >= 0 ? theta : -theta);\n"
"COMPUTE_FORCE\n"
"real normCross1 = dot(cp0, cp0);\n"
"real normSqrBC = dot(v1, v1);\n"
"real normBC = SQRT(normSqrBC);\n"
"real normCross2 = dot(cp1, cp1);\n"
"real dp = RECIP(normSqrBC);\n"
"real4 ff = make_real4((-dEdAngle*normBC)/normCross1, dot(v0, v1)*dp, dot(v2, v1)*dp, (dEdAngle*normBC)/normCross2);\n"
"real3 force1 = ff.x*cp0;\n"
"real3 force4 = ff.w*cp1;\n"
"real3 s = ff.y*force1 - ff.z*force4;\n"
"real3 force2 = s-force1;\n"
"real3 force3 = -s-force4;\n"
"";
const string CudaKernelSources::utilities = "extern \"C\" {\n"
"\n"
"/**\n"
" * This is called by the various functions below to clear a buffer.\n"
" */\n"
"__device__ void clearSingleBuffer(int* __restrict__ buffer, int size) {\n"
"    int index = blockDim.x*blockIdx.x+threadIdx.x;\n"
"    int4* buffer4 = (int4*) buffer;\n"
"    int sizeDiv4 = size/4;\n"
"    while (index < sizeDiv4) {\n"
"        buffer4[index] = make_int4(0);\n"
"        index += blockDim.x*gridDim.x;\n"
"    }\n"
"    if (blockDim.x*blockIdx.x+threadIdx.x == 0)\n"
"        for (int i = sizeDiv4*4; i < size; i++)\n"
"            buffer[i] = 0;\n"
"}\n"
"\n"
"/**\n"
" * Fill a buffer with 0.\n"
" */\n"
"__global__ void clearBuffer(int* __restrict__ buffer, int size) {\n"
"    clearSingleBuffer(buffer, size);\n"
"}\n"
"\n"
"/**\n"
" * Fill two buffers with 0.\n"
" */\n"
"__global__ void clearTwoBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"}\n"
"\n"
"/**\n"
" * Fill three buffers with 0.\n"
" */\n"
"__global__ void clearThreeBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"}\n"
"\n"
"/**\n"
" * Fill four buffers with 0.\n"
" */\n"
"__global__ void clearFourBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"}\n"
"\n"
"/**\n"
" * Fill five buffers with 0.\n"
" */\n"
"__global__ void clearFiveBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4, int* __restrict__ buffer5, int size5) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"    clearSingleBuffer(buffer5, size5);\n"
"}\n"
"\n"
"/**\n"
" * Fill six buffers with 0.\n"
" */\n"
"__global__ void clearSixBuffers(int* __restrict__ buffer1, int size1, int* __restrict__ buffer2, int size2, int* __restrict__ buffer3, int size3, int* __restrict__ buffer4, int size4, int* __restrict__ buffer5, int size5, int* __restrict__ buffer6, int size6) {\n"
"    clearSingleBuffer(buffer1, size1);\n"
"    clearSingleBuffer(buffer2, size2);\n"
"    clearSingleBuffer(buffer3, size3);\n"
"    clearSingleBuffer(buffer4, size4);\n"
"    clearSingleBuffer(buffer5, size5);\n"
"    clearSingleBuffer(buffer6, size6);\n"
"}\n"
"\n"
"/**\n"
" * Sum the energy buffer.\n"
" */\n"
"__global__ void reduceEnergy(const mixed* __restrict__ energyBuffer, mixed* __restrict__ result, int bufferSize, int workGroupSize) {\n"
"    extern __shared__ mixed tempBuffer[];\n"
"    const unsigned int thread = threadIdx.x;\n"
"    mixed sum = 0;\n"
"    for (unsigned int index = thread; index < bufferSize; index += blockDim.x)\n"
"        sum += energyBuffer[index];\n"
"    tempBuffer[thread] = sum;\n"
"    for (int i = 1; i < workGroupSize; i *= 2) {\n"
"        __syncthreads();\n"
"        if (thread%(i*2) == 0 && thread+i < workGroupSize)\n"
"            tempBuffer[thread] += tempBuffer[thread+i];\n"
"    }\n"
"    if (thread == 0)\n"
"        *result = tempBuffer[0];\n"
"}\n"
"\n"
"/**\n"
" * Record the atomic charges into the posq array.\n"
" */\n"
"__global__ void setCharges(real* __restrict__ charges, real4* __restrict__ posq, int* __restrict__ atomOrder, int numAtoms) {\n"
"    for (int i = blockDim.x*blockIdx.x+threadIdx.x; i < numAtoms; i += blockDim.x*gridDim.x)\n"
"        posq[i].w = charges[atomOrder[i]];\n"
"}\n"
"}\n"
"";
const string CudaKernelSources::vectorOps = "/**\n"
" * This file defines vector operations to simplify code elsewhere.\n"
" */\n"
"\n"
"// Versions of make_x() that take a single value and set all components to that.\n"
"\n"
"inline __device__ int2 make_int2(int a) {\n"
"    return make_int2(a, a);\n"
"}\n"
"\n"
"inline __device__ int3 make_int3(int a) {\n"
"    return make_int3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ int4 make_int4(int a) {\n"
"    return make_int4(a, a, a, a);\n"
"}\n"
"\n"
"inline __device__ float2 make_float2(float a) {\n"
"    return make_float2(a, a);\n"
"}\n"
"\n"
"inline __device__ float3 make_float3(float a) {\n"
"    return make_float3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ float4 make_float4(float a) {\n"
"    return make_float4(a, a, a, a);\n"
"}\n"
"\n"
"inline __device__ double2 make_double2(double a) {\n"
"    return make_double2(a, a);\n"
"}\n"
"\n"
"inline __device__ double3 make_double3(double a) {\n"
"    return make_double3(a, a, a);\n"
"}\n"
"\n"
"inline __device__ double4 make_double4(double a) {\n"
"    return make_double4(a, a, a, a);\n"
"}\n"
"\n"
"// Negate a vector.\n"
"\n"
"inline __device__ int2 operator-(int2 a) {\n"
"    return make_int2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator-(int3 a) {\n"
"    return make_int3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator-(int4 a) {\n"
"    return make_int4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator-(float2 a) {\n"
"    return make_float2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator-(float3 a) {\n"
"    return make_float3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator-(float4 a) {\n"
"    return make_float4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator-(double2 a) {\n"
"    return make_double2(-a.x, -a.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator-(double3 a) {\n"
"    return make_double3(-a.x, -a.y, -a.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator-(double4 a) {\n"
"    return make_double4(-a.x, -a.y, -a.z, -a.w);\n"
"}\n"
"\n"
"// Add two vectors.\n"
"\n"
"inline __device__ int2 operator+(int2 a, int2 b) {\n"
"    return make_int2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator+(int3 a, int3 b) {\n"
"    return make_int3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator+(int4 a, int4 b) {\n"
"    return make_int4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator+(float2 a, float2 b) {\n"
"    return make_float2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator+(float3 a, float3 b) {\n"
"    return make_float3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator+(float4 a, float4 b) {\n"
"    return make_float4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator+(double2 a, double2 b) {\n"
"    return make_double2(a.x+b.x, a.y+b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator+(double3 a, double3 b) {\n"
"    return make_double3(a.x+b.x, a.y+b.y, a.z+b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator+(double4 a, double4 b) {\n"
"    return make_double4(a.x+b.x, a.y+b.y, a.z+b.z, a.w+b.w);\n"
"}\n"
"\n"
"// Subtract two vectors.\n"
"\n"
"inline __device__ int2 operator-(int2 a, int2 b) {\n"
"    return make_int2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator-(int3 a, int3 b) {\n"
"    return make_int3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator-(int4 a, int4 b) {\n"
"    return make_int4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator-(float2 a, float2 b) {\n"
"    return make_float2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator-(float3 a, float3 b) {\n"
"    return make_float3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator-(float4 a, float4 b) {\n"
"    return make_float4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator-(double2 a, double2 b) {\n"
"    return make_double2(a.x-b.x, a.y-b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator-(double3 a, double3 b) {\n"
"    return make_double3(a.x-b.x, a.y-b.y, a.z-b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator-(double4 a, double4 b) {\n"
"    return make_double4(a.x-b.x, a.y-b.y, a.z-b.z, a.w-b.w);\n"
"}\n"
"\n"
"// Multiply two vectors.\n"
"\n"
"inline __device__ int2 operator*(int2 a, int2 b) {\n"
"    return make_int2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int3 a, int3 b) {\n"
"    return make_int3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int4 a, int4 b) {\n"
"    return make_int4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float2 a, float2 b) {\n"
"    return make_float2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float3 a, float3 b) {\n"
"    return make_float3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float4 a, float4 b) {\n"
"    return make_float4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double2 a, double2 b) {\n"
"    return make_double2(a.x*b.x, a.y*b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double3 a, double3 b) {\n"
"    return make_double3(a.x*b.x, a.y*b.y, a.z*b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double4 a, double4 b) {\n"
"    return make_double4(a.x*b.x, a.y*b.y, a.z*b.z, a.w*b.w);\n"
"}\n"
"\n"
"// Divide two vectors.\n"
"\n"
"inline __device__ int2 operator/(int2 a, int2 b) {\n"
"    return make_int2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator/(int3 a, int3 b) {\n"
"    return make_int3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator/(int4 a, int4 b) {\n"
"    return make_int4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator/(float2 a, float2 b) {\n"
"    return make_float2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator/(float3 a, float3 b) {\n"
"    return make_float3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator/(float4 a, float4 b) {\n"
"    return make_float4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator/(double2 a, double2 b) {\n"
"    return make_double2(a.x/b.x, a.y/b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator/(double3 a, double3 b) {\n"
"    return make_double3(a.x/b.x, a.y/b.y, a.z/b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator/(double4 a, double4 b) {\n"
"    return make_double4(a.x/b.x, a.y/b.y, a.z/b.z, a.w/b.w);\n"
"}\n"
"\n"
"// += operator\n"
"\n"
"inline __device__ void operator+=(int2& a, int2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(int3& a, int3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(int4& a, int4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float2& a, float2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float3& a, float3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(float4& a, float4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double2& a, double2 b) {\n"
"    a.x += b.x; a.y += b.y;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double3& a, double3 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z;\n"
"}\n"
"\n"
"inline __device__ void operator+=(double4& a, double4 b) {\n"
"    a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w;\n"
"}\n"
"\n"
"// -= operator\n"
"\n"
"inline __device__ void operator-=(int2& a, int2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(int3& a, int3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(int4& a, int4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float2& a, float2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float3& a, float3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(float4& a, float4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double2& a, double2 b) {\n"
"    a.x -= b.x; a.y -= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double3& a, double3 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator-=(double4& a, double4 b) {\n"
"    a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w;\n"
"}\n"
"\n"
"// *= operator\n"
"\n"
"inline __device__ void operator*=(int2& a, int2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int3& a, int3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int4& a, int4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float2& a, float2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float3& a, float3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float4& a, float4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double2& a, double2 b) {\n"
"    a.x *= b.x; a.y *= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double3& a, double3 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double4& a, double4 b) {\n"
"    a.x *= b.x; a.y *= b.y; a.z *= b.z; a.w *= b.w;\n"
"}\n"
"\n"
"// /= operator\n"
"\n"
"inline __device__ void operator/=(int2& a, int2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(int3& a, int3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(int4& a, int4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float2& a, float2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float3& a, float3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(float4& a, float4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double2& a, double2 b) {\n"
"    a.x /= b.x; a.y /= b.y;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double3& a, double3 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z;\n"
"}\n"
"\n"
"inline __device__ void operator/=(double4& a, double4 b) {\n"
"    a.x /= b.x; a.y /= b.y; a.z /= b.z; a.w /= b.w;\n"
"}\n"
"\n"
"// Multiply a vector by a constant.\n"
"\n"
"inline __device__ int2 operator*(int2 a, int b) {\n"
"    return make_int2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int3 a, int b) {\n"
"    return make_int3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int4 a, int b) {\n"
"    return make_int4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ int2 operator*(int a, int2 b) {\n"
"    return make_int2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ int3 operator*(int a, int3 b) {\n"
"    return make_int3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ int4 operator*(int a, int4 b) {\n"
"    return make_int4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float2 a, float b) {\n"
"    return make_float2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float3 a, float b) {\n"
"    return make_float3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float4 a, float b) {\n"
"    return make_float4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ float2 operator*(float a, float2 b) {\n"
"    return make_float2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ float3 operator*(float a, float3 b) {\n"
"    return make_float3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ float4 operator*(float a, float4 b) {\n"
"    return make_float4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double2 a, double b) {\n"
"    return make_double2(a.x*b, a.y*b);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double3 a, double b) {\n"
"    return make_double3(a.x*b, a.y*b, a.z*b);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double4 a, double b) {\n"
"    return make_double4(a.x*b, a.y*b, a.z*b, a.w*b);\n"
"}\n"
"\n"
"inline __device__ double2 operator*(double a, double2 b) {\n"
"    return make_double2(a*b.x, a*b.y);\n"
"}\n"
"\n"
"inline __device__ double3 operator*(double a, double3 b) {\n"
"    return make_double3(a*b.x, a*b.y, a*b.z);\n"
"}\n"
"\n"
"inline __device__ double4 operator*(double a, double4 b) {\n"
"    return make_double4(a*b.x, a*b.y, a*b.z, a*b.w);\n"
"}\n"
"\n"
"// Divide a vector by a constant.\n"
"\n"
"inline __device__ int2 operator/(int2 a, int b) {\n"
"    return make_int2(a.x/b, a.y/b);\n"
"}\n"
"\n"
"inline __device__ int3 operator/(int3 a, int b) {\n"
"    return make_int3(a.x/b, a.y/b, a.z/b);\n"
"}\n"
"\n"
"inline __device__ int4 operator/(int4 a, int b) {\n"
"    return make_int4(a.x/b, a.y/b, a.z/b, a.w/b);\n"
"}\n"
"\n"
"inline __device__ float2 operator/(float2 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ float3 operator/(float3 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ float4 operator/(float4 a, float b) {\n"
"    float scale = 1.0f/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double2 operator/(double2 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double3 operator/(double3 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"inline __device__ double4 operator/(double4 a, double b) {\n"
"    double scale = 1.0/b;\n"
"    return a*scale;\n"
"}\n"
"\n"
"// *= operator (multiply vector by constant)\n"
"\n"
"inline __device__ void operator*=(int2& a, int b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int3& a, int b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(int4& a, int b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float2& a, float b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float3& a, float b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(float4& a, float b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double2& a, double b) {\n"
"    a.x *= b; a.y *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double3& a, double b) {\n"
"    a.x *= b; a.y *= b; a.z *= b;\n"
"}\n"
"\n"
"inline __device__ void operator*=(double4& a, double b) {\n"
"    a.x *= b; a.y *= b; a.z *= b; a.w *= b;\n"
"}\n"
"\n"
"// Dot product\n"
"\n"
"inline __device__ float dot(float3 a, float3 b) {\n"
"    return a.x*b.x+a.y*b.y+a.z*b.z;\n"
"}\n"
"\n"
"inline __device__ double dot(double3 a, double3 b) {\n"
"    return a.x*b.x+a.y*b.y+a.z*b.z;\n"
"}\n"
"\n"
"// Cross product\n"
"\n"
"inline __device__ float3 cross(float3 a, float3 b) {\n"
"    return make_float3(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x);\n"
"}\n"
"\n"
"inline __device__ float4 cross(float4 a, float4 b) {\n"
"    return make_float4(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x, 0.0f);\n"
"}\n"
"\n"
"inline __device__ double3 cross(double3 a, double3 b) {\n"
"    return make_double3(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x);\n"
"}\n"
"\n"
"inline __device__ double4 cross(double4 a, double4 b) {\n"
"    return make_double4(a.y*b.z-a.z*b.y, a.z*b.x-a.x*b.z, a.x*b.y-a.y*b.x, 0.0);\n"
"}\n"
"\n"
"// Normalize a vector\n"
"\n"
"inline __device__ float2 normalize(float2 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y);\n"
"}\n"
"\n"
"inline __device__ float3 normalize(float3 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y+a.z*a.z);\n"
"}\n"
"\n"
"inline __device__ float4 normalize(float4 a) {\n"
"    return a*rsqrtf(a.x*a.x+a.y*a.y+a.z*a.z+a.w*a.w);\n"
"}\n"
"\n"
"inline __device__ double2 normalize(double2 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y);\n"
"}\n"
"\n"
"inline __device__ double3 normalize(double3 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y+a.z*a.z);\n"
"}\n"
"\n"
"inline __device__ double4 normalize(double4 a) {\n"
"    return a*rsqrt(a.x*a.x+a.y*a.y+a.z*a.z+a.w*a.w);\n"
"}\n"
"\n"
"// Strip off the fourth component of a vector.\n"
"\n"
"inline __device__ short3 trimTo3(short4 v) {\n"
"    return make_short3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ int3 trimTo3(int4 v) {\n"
"    return make_int3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ float3 trimTo3(float4 v) {\n"
"    return make_float3(v.x, v.y, v.z);\n"
"}\n"
"\n"
"inline __device__ double3 trimTo3(double4 v) {\n"
"    return make_double3(v.x, v.y, v.z);\n"
"}\n"
"";
